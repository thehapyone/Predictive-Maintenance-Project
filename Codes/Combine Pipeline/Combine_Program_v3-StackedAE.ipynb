{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Combine_Program_v3-StackedAE.ipynb","provenance":[{"file_id":"1BoVamoDHf-RcqzCXQFiKBOoOa4i63ZLJ","timestamp":1587472173034},{"file_id":"1CaG5QvYHTcQ2UcZ9l9VKRJoVIIpv4UqZ","timestamp":1586790227987},{"file_id":"1NvNrfS6WNfu7LaUt8qQxJpbeyYcYjKjf","timestamp":1586535304233},{"file_id":"1e8iRIrJdfiXaFiz0EnlnZg5fIv3pPGKL","timestamp":1585856613340},{"file_id":"1AFawAHwK2dVU6tytPdDddcBPycO4gKdM","timestamp":1585761860398},{"file_id":"1x9RL5J2ixg0oo-tABiFY_6lNz6lv9KGp","timestamp":1585731097471},{"file_id":"1K2xLSCfIUr9Y_a4IRMKpS_BCu2CX9Q56","timestamp":1585500139565},{"file_id":"1dNbjhBsFetQAx_cu72cb-256k1TR8XEl","timestamp":1585224618326}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UBH4RjYX1OGw","colab_type":"text"},"source":["# Main Program for Numerical and Categorical Feature\n","This program combines the models built for both the numerical and categorical features"]},{"cell_type":"markdown","metadata":{"id":"1kS2iuLi1tCV","colab_type":"text"},"source":["function ClickConnect(){\n","console.log(\"Working\");\n","document.querySelector(\"colab-connect-button\").shadowRoot.getElementById('connect').click()\n","}\n","setInterval(ClickConnect,60000)"]},{"cell_type":"code","metadata":{"id":"aAQD2JtYkD2v","colab_type":"code","colab":{}},"source":["# Seed value\n","# Apparently you may use different seed values at each stage\n","seed_value= 5\n","\n","# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","# 2. Set the `python` built-in pseudo-random generator at a fixed value\n","import random\n","random.seed(seed_value)\n","\n","# 3. Set the `numpy` pseudo-random generator at a fixed value\n","import numpy as np\n","np.random.seed(seed_value)\n","\n","# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n","import tensorflow as tf\n","tf.random.set_seed(seed_value)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7mCtb7J1TX8","colab_type":"code","outputId":"6ddfff50-9d74-4b66-f95c-35b198576d6f","executionInfo":{"status":"ok","timestamp":1587472233829,"user_tz":-120,"elapsed":33025,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kc1JPb4v074V","colab_type":"code","outputId":"c872e0b2-dc3f-472c-d270-d0c58f7a4136","executionInfo":{"status":"ok","timestamp":1587472233830,"user_tz":-120,"elapsed":33014,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import copy as copy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FI-9fGnufnoN","colab_type":"code","colab":{}},"source":["# use tensorflow version 2\n","%tensorflow_version 2.x\n","\n","# load the encoder model to be used\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Embedding, Input, Reshape, Concatenate, Dense, Flatten, Dropout\n","\n","from tensorflow.keras.callbacks import TensorBoard\n","import os\n","import datetime\n","\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.callbacks import EarlyStopping"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZslDysrzghEL","colab_type":"code","outputId":"defbebda-4b55-4b1b-ca6d-f106b0641d10","executionInfo":{"status":"ok","timestamp":1587472233832,"user_tz":-120,"elapsed":33004,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from tensorflow.keras.layers import PReLU, ReLU, BatchNormalization, ELU\n","from keras.initializers import Constant\n","from keras import regularizers\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gsk9Eigsjn5M","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import StandardScaler, RobustScaler\n","from sklearn.metrics import mean_absolute_error, explained_variance_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oD7nqTYf6a2","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","# for cross validation, we can use Group KFold spliting - GroupShuffleSplit\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BRGnXWAW-rXj","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('drive/My Drive/Thesis/Collab Notebooks/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VEwTzseoK3F","colab_type":"code","colab":{}},"source":["# Applying the Extraction pipeline\n","# first stage of the pipeline\n","from extraction_pipeline import *\n","from category_pipeline import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjSOi7PZ1VFx","colab_type":"code","colab":{}},"source":["# using the latest updated file\n","#data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v3_Sorted_Raw_Db_with_RUL.csv\", sep=',', low_memory=False)\n","#ata = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v4_Sorted_Raw_Db_with_RUL_imputed.csv\", sep=',', low_memory=False)\n","#data = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/sorted_data_new_full.csv\", sep=',', low_memory=False)\n","#data = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/sorted_data_new.csv\", sep=',', low_memory=False)\n","data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v5_Sorted_Raw_Db_with_RUL_imputed.csv\", sep=',', low_memory=False)\n","data2 = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/v2_sorted_data_new.csv\", sep=',', low_memory=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d14TT33eD3Qb","colab_type":"text"},"source":["## Category Pipeline\n"]},{"cell_type":"code","metadata":{"id":"EsHVm38hD5V_","colab_type":"code","outputId":"e8b0da2f-7750-413c-b2d6-8d9cf213d9ef","executionInfo":{"status":"ok","timestamp":1587472482541,"user_tz":-120,"elapsed":281692,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Date transformation\n","data = extraTrasfromToCategory(data, debug=True)\n","\n","'''\n","# Zero Variance removal\n","data = removeZeroCategory(data)\n","print ('After all removal pipeline - ',data.shape)\n","'''"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Dtypes are: \n","DAY                                  object\n","MONTH                                object\n","YEAR                                 object\n","VFE_0005_VEHICLE_OPERATION_DIGIT1    object\n","VFE_0006_VEHICLE_OPERATION_DIGIT2    object\n","VFE_0008_HAS_PTO                     object\n","dtype: object\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["\"\\n# Zero Variance removal\\ndata = removeZeroCategory(data)\\nprint ('After all removal pipeline - ',data.shape)\\n\""]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"uF2B3mwOqYOD","colab_type":"code","colab":{}},"source":["# load this information from the save csv file\n","cat_features_df = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_embedding_details.csv\", sep=',', low_memory=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3OwJu5BMlcU","colab_type":"code","outputId":"d1d907bd-deaf-409f-97bf-17638fe45313","executionInfo":{"status":"ok","timestamp":1587472483006,"user_tz":-120,"elapsed":282146,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["data.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(19954, 447)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"JyCRLRt0TouM","colab_type":"text"},"source":["(336677, 449)"]},{"cell_type":"code","metadata":{"id":"7B7KXV8TMb4P","colab_type":"code","colab":{}},"source":["# load feature names from db\n","feature_names_cat = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_cat.csv\", sep=',', low_memory=False)\n","feature_names_num = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_num.csv\", sep=',', low_memory=False)\n","# fetch the categorical and numerical features\n","cat_features = feature_names_cat['Categorical'].values\n","num_features = feature_names_num['Numerical'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlkJq-7lPpRw","colab_type":"code","outputId":"099047f1-07a7-4097-e215-baae38db8e7c","executionInfo":{"status":"ok","timestamp":1587472484127,"user_tz":-120,"elapsed":283257,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["num_features.shape, cat_features.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((362,), (74,))"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"W9Y43CX2QWmC","colab_type":"code","colab":{}},"source":["# Function for assisting in spliting the data\n","def validationSplit(data_in, split=0.2, toShuffle=False):\n","  # here we will split the data\n","  # splitting by chassis\n","  chassis_data = data_in['T_CHASSIS'].unique()\n","  # splitting the chassis data\n","  c_train, c_val = train_test_split(chassis_data, test_size=split, random_state=42, shuffle=toShuffle)\n","  # extracting out the training and testing\n","  train_data = data_in[data_in['T_CHASSIS'].isin(c_train)].reset_index(drop=True)\n","  test_data = data_in[data_in['T_CHASSIS'].isin(c_val)].reset_index(drop=True)\n","  return train_data, test_data\n","\n","# Function for assisting in spliting the data\n","def crossValidationSplit(kfolds=5, split=0.2, toShuffle=False, ):\n","  # here we will split the data based on their chassis grouping\n","  ## generating the Group Parameters\n","  c_split = GroupShuffleSplit(n_splits=kfolds, test_size=0.2, random_state=42)\n","  '''\n","  # splitting the data\n","  for train_idx, test_idx in c_split.split(data.values, groups=chassis_group):\n","    print(\"TRAIN:\", data.values[train_idx].shape, \"TEST:\", data.values[test_idx].shape)\n","  '''\n","  return c_split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5lzJJLj29fn6","colab_type":"text"},"source":["### Numerical Pipeline"]},{"cell_type":"markdown","metadata":{"id":"09AZMOXA7_ra","colab_type":"text"},"source":["#### Learning Pipeline"]},{"cell_type":"code","metadata":{"id":"IL6XyCKm8ymo","colab_type":"code","colab":{}},"source":["# here we will load the learned pipeline selected features\n","# load feature names from db\n","feature_names_learning = pd.read_csv(\"/content/drive/My Drive/Thesis/data/Learning Pipeline/Best_Features_from_Feature_Selection.csv\", sep=',', low_memory=False)\n","# fetch the learned features\n","learned_features = feature_names_learning['Best_100_Feature Names'].values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASiNt8Sf9ZN0","colab_type":"text"},"source":["#### Encoder Pipeline"]},{"cell_type":"markdown","metadata":{"id":"ZSq7A1eMHAji","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"O6vkc4d-h7VN","colab_type":"code","outputId":"ff7c39d6-dc59-44d2-d88d-4383392efc5e","executionInfo":{"status":"ok","timestamp":1587472484129,"user_tz":-120,"elapsed":283247,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","\n","def create_num_models():\n","  # load the embedding model\n","  # identical to the previous one\n","  vanila_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Vanilla_AE_20.h5\")\n","  denoising_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Denoising_AE.h5\")\n","\n","  # extract out the vanila model\n","  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\n","  output_layer = vanila_num_model.get_layer(name='bottleneck_layer')(en_input_layer)\n","  # create the encoder model for the vanila model\n","  encoder_vanila = Model(inputs=en_input_layer, outputs=output_layer, name='20_features_Vanila_Encoder')\n","\n","  # extract out the denoising model\n","  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\n","  enc_layer_1 = denoising_num_model.get_layer(name='enc_layer_1')(en_input_layer)\n","  enc_layer_2 = denoising_num_model.get_layer(name='enc_layer_2')(enc_layer_1)\n","  enc_layer_3 = denoising_num_model.get_layer(name='enc_layer_3')(enc_layer_2)\n","\n","  #output_layer = vanila_num_model.get_layer(name='bottleneck_layer')(en_input_layer)\n","  # create the encoder model for the denosing model\n","  encoder_denosing = Model(inputs=en_input_layer, outputs=enc_layer_3, name='25_features_denosing_Encoder')\n","\n","  return encoder_vanila, encoder_denosing\n","\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\ndef create_num_models():\\n  # load the embedding model\\n  # identical to the previous one\\n  vanila_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Vanilla_AE_20.h5\")\\n  denoising_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Denoising_AE.h5\")\\n\\n  # extract out the vanila model\\n  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\\n  output_layer = vanila_num_model.get_layer(name=\\'bottleneck_layer\\')(en_input_layer)\\n  # create the encoder model for the vanila model\\n  encoder_vanila = Model(inputs=en_input_layer, outputs=output_layer, name=\\'20_features_Vanila_Encoder\\')\\n\\n  # extract out the denoising model\\n  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\\n  enc_layer_1 = denoising_num_model.get_layer(name=\\'enc_layer_1\\')(en_input_layer)\\n  enc_layer_2 = denoising_num_model.get_layer(name=\\'enc_layer_2\\')(enc_layer_1)\\n  enc_layer_3 = denoising_num_model.get_layer(name=\\'enc_layer_3\\')(enc_layer_2)\\n\\n  #output_layer = vanila_num_model.get_layer(name=\\'bottleneck_layer\\')(en_input_layer)\\n  # create the encoder model for the denosing model\\n  encoder_denosing = Model(inputs=en_input_layer, outputs=enc_layer_3, name=\\'25_features_denosing_Encoder\\')\\n\\n  return encoder_vanila, encoder_denosing\\n\\n'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"78egEozwhuJT","colab_type":"code","colab":{}},"source":["# create the numerical encoding model\n","\n","#encoder_vanila, encoder_denosing = create_num_models()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KjtjGm6zsNn_","colab_type":"code","colab":{}},"source":["encoder_vanila_50 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_50.h5\",compile= False)\n","encoder_vanila_40 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_40.h5\",compile= False)\n","encoder_vanila_30 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_30.h5\",compile= False)\n","\n","encoder_denosing = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Denoising_Encoder.h5\",compile= False)\n","encoder_deep = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Deep_Encoder.h5\",compile= False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uP7HVJIkehjb","colab_type":"code","colab":{}},"source":["# this function manages the numerical stuffs\n","# given a data (scaled), it will generate a new representation of that data\n","def numerical_pipeline(data_in=None, model1=None, model2=None):\n","  # first extract out the learning pipeline\n","  data_num = data_in[learned_features].values\n","\n","  # the encoder model as well - first model\n","  encoder_1_data = model1.predict(data_num)\n","  # encoder_2_data = model2.predict(data_num)\n","\n","  # merge them together\n","  # encoder_num = np.concatenate((encoder_1_data, encoder_2_data), axis=1)\n","  \n","  # that's all here\n","  return encoder_1_data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9rMSkVNE_gr8","colab_type":"text"},"source":["### Category Pipeline"]},{"cell_type":"markdown","metadata":{"id":"twp6hyyLNLHg","colab_type":"text"},"source":["In summary, we have a total of 74 categories. "]},{"cell_type":"markdown","metadata":{"id":"S4FSV3zNC3Q5","colab_type":"text"},"source":["#### Extras"]},{"cell_type":"code","metadata":{"id":"VWQrVg012oYE","colab_type":"code","colab":{}},"source":["# Goal\n","'''\n","Write a function, when given a categorical variable, it will generate the vector representation for that variable.\n","In order to achieve that, we first need to map very categorical variable to some numerical index value. This value \n","has to be unique and not random. \n","'''\n","\n","def transfromForEmbed(data_cat, cat_feature_name='', cat_mapping=''):\n","  # find the cat_index for all the features\n","  def findCatIndex(x):\n","    return int(cat_mapping[cat_mapping[cat_feature_name] == x][cat_feature_name+'_index'].values[0])\n","  output = data_cat[cat_feature_name].apply(findCatIndex) \n","  return output\n","\n","def embedTransform(model=None, variable='', cat_feature_name='', cat_mapping='', return_type=1):\n","  # Given a categorical variable, produce the embedded vector\n","  # Find the index of the categorical variable in the cat_mapping dataframe\n","  cat_index = int(cat_mapping[cat_mapping[cat_feature_name] == variable][cat_feature_name+'_index'].values[0])\n","  if return_type == 0:\n","    return cat_index\n","  # fetch the embeddings weights\n","  pre_embedding = \"Embedding_layer_\"\n","  embed_layer = model.get_layer(name=pre_embedding+cat_feature_name)\n","  embedd_vector = embed_layer.get_weights()[0][cat_index]\n","  return embedd_vector\n","\n","def createCatMapping(data_cat):\n","  # this function will create a mapping dataframe\n","  cat_size = data_cat.shape[1]\n","  feature_names = data_cat.columns\n","\n","  cat_mapping = pd.DataFrame()\n","\n","  col_mapping_list = list()\n","  col_mapping_cols = list()\n","  # try for all dataframes\n","  for i in range(cat_size):\n","    col_indexs = list(pd.factorize(data_cat.iloc[:, i].unique())[0])\n","    # save to list\n","    col_mapping_list.append(list(data_cat.iloc[:, i].unique()))\n","    col_mapping_list.append(col_indexs)\n","    # save the column name as well\n","    col_mapping_cols.append(feature_names[i])\n","    col_mapping_cols.append(feature_names[i]+'_index')\n","\n","  # now we need to save the list to a dataframe\n","  cat_mapping = pd.DataFrame(col_mapping_list)\n","  # transpose it\n","  cat_mapping = cat_mapping.transpose()\n","  # update the columns names\n","  cat_mapping.columns = col_mapping_cols\n","\n","  # save to the dataframe\n","  cat_mapping.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', index=False)\n","\n","#createCatMapping(data[cat_features]) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3ZS29ptVFn6s","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","# Clear any logs from previous runs\n","!rm -rf ./logs/ \n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","#tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1, embeddings_metadata=embeddings_meta, write_images=True)\n","tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1,\n","                                   write_images=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6JUt7abZXbW","colab_type":"code","colab":{}},"source":["### Creating the Keras Embeddings Mode\n","\n","def create_model(saved_model=None):\n","  # IDs representing 1-hot encodings\n","  # Need to create the input for all features\n","  cat_embedding_input_layers = []\n","\n","  cat_reshape_layers = []\n","  # Embeddings for the first cat feature\n","  id_feature = 0\n","  # create an array of embeddings layers\n","  cat_embedding_layers = []\n","\n","  # interate through categorical varaibles\n","  for id_feature in range(cat_features_df.shape[0]):\n","    # creating the input layer for the embeddings\n","    #input_layer = Input(shape=(1,), name=\"Input_layer_\"+cat_features_df.loc[id_feature, 'Feature'])\n","    input_layer = Input(shape=(1,), name=\"Input_layer_\"+cat_features_df.loc[id_feature, 'Feature'])\n","    cat_embedding_input_layers.append(input_layer)\n","    # embedding size \n","    layer_embedding_size = cat_features_df.loc[id_feature, 'Embedding_Size']\n","    # create the embedding layers\n","    embed_weights = saved_model.get_layer(name=\"Embedding_layer_\"+cat_features_df.loc[id_feature, 'Feature']).get_weights()\n","    #embedded_layer.trainable = False\n","    embedded_layer = Embedding(input_dim=cat_features_df.loc[id_feature, 'Cardinality'], output_dim=layer_embedding_size, \n","                               name=\"Embedding_layer_\"+cat_features_df.loc[id_feature, 'Feature'], input_length = 1, trainable=False, weights=embed_weights)(input_layer)\n","    cat_embedding_layers.append(embedded_layer)\n","    # add a reshape of the embedding layers\n","    reshape_layer = Reshape(target_shape=(layer_embedding_size,))(embedded_layer)\n","    # appends the rehshape models together\n","    cat_reshape_layers.append(reshape_layer)\n","\n","  ### Create A combined embedding layers only\n","  combined_emb = Concatenate(axis=1, name='combined_embeddings')(cat_reshape_layers)\n","  \n","  ### the output of the combine embedding model is 169 vectors\n","\n","  ### creating a model based on the trained weights\n","  model_emb = Model(inputs=cat_embedding_input_layers, outputs=combined_emb, name=\"Categorical_Embeddings\")\n","\n","  # compile the model - for a regression model\n","  model_emb.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])\n","  # compile the model - for a classification problem\n","  # model.compile(loss='binary_crossentropy', optimizer='adam')\n","  return model_emb\n","\n","def train_model(model, x_train, y_train, x_test, y_test, epochs, batch_size, early_stop_callback=None, tensorboard_callback=None):\n","  # fitting the model\n","  #model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_data=(XTest, yTest), callbacks=[early_stop, tensorboard_callback])\n","  model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[early_stop_callback, tensorboard_callback])\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4WFSz0kSuuC","colab_type":"code","colab":{}},"source":["## For each run, log an hparams summary with the hyperparameters and final accuracy:\n","def run(my_model, x_train, y_train, epochs, batch_size, early_stop_callback, tensorboard_log):\n","  model = None\n","  model = create_model(my_model)\n","  model_output = train_model(model, x_train, y_train, epochs, batch_size, early_stop_callback=early_stop_callback, tensorboard_callback=tensorboard_log)\n","  # do something with the metrics\n","  return model_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbuVc8kGWYqV","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import RobustScaler, MinMaxScaler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUlRi7_CozT4","colab_type":"code","colab":{}},"source":["def transfromForEmbed(data_cat, cat_feature_name='', cat_mapping='', type=None):\n","  # find the cat_index for all the features\n","  def findCatIndex(x):\n","    return int(cat_mapping[cat_mapping[cat_feature_name] == x][cat_feature_name+'_index'].values[0])\n","  if type is not None:\n","    output = findCatIndex(data_cat[cat_feature_name])\n","    return np.array([output])\n","  else:\n","    output = data_cat[cat_feature_name].apply(findCatIndex) \n","    return output.values\n","\n","# function to reformat the input data into the necessary input data for our network to be trained on.\n","def network_input_process(data_num=None, data_cols=None, y_output=None, input_type=1):\n","  # create a list of inputs. \n","  try:\n","    cat_size = data_cols.shape[1]\n","    columns_name = data_cols.columns\n","    col_type = None\n","  except:\n","    cat_size = data_cols.shape[0]\n","    columns_name = data_cols.index\n","    col_type = 1\n","\n","  # define the size of the network list inputs\n","  network_inputs = [None] * (cat_size)\n","\n","  # add inputs to the list\n","  # for the categorical inputs\n","\n","  for i in range(cat_size):\n","    #network_inputs[i] = pd.factorize(data_cols.iloc[:, i].values)[0]\n","    # map each categorical variable to their respective index values\n","    #print (columns_name[i])\n","    #network_inputs[i] = transfromForEmbed(data_cols, cat_feature_name=columns_name[i], cat_mapping=cat_mappings, type=col_type)\n","    network_inputs[i] = data_cols.loc[:, columns_name[i]].values\n","\n","  return network_inputs, y_output.values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_hvo4n8st89","colab_type":"code","outputId":"7164db2b-87dd-44a6-d29a-8dc07f1ceac4","executionInfo":{"status":"ok","timestamp":1587472490068,"user_tz":-120,"elapsed":289155,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["'''\n","# creating the training inputs\n","scaler = RobustScaler()\n","# scale and fit\n","#scaler.fit(data_t.loc[:, num_features].values)\n","#data_t.loc[:, num_features] = scaler.transform(data_t.loc[:, num_features].values)\n","#data_v.loc[:, num_features] = scaler.transform(data_v.loc[:, num_features].values)\n","\n","# create the requirment for the embedding model\n","XTrain, yTrain = network_input_process(data_num=None, data_cols=data[cat_features], y_output=data.iloc[:,3])\n","'''"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# creating the training inputs\\nscaler = RobustScaler()\\n# scale and fit\\n#scaler.fit(data_t.loc[:, num_features].values)\\n#data_t.loc[:, num_features] = scaler.transform(data_t.loc[:, num_features].values)\\n#data_v.loc[:, num_features] = scaler.transform(data_v.loc[:, num_features].values)\\n\\n# create the requirment for the embedding model\\nXTrain, yTrain = network_input_process(data_num=None, data_cols=data[cat_features], y_output=data.iloc[:,3])\\n'"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"5jmf037rcf2H","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"pXfQPjtd2Heq","colab_type":"text"},"source":["#### Embeddings"]},{"cell_type":"markdown","metadata":{"id":"ODkykfeJQUuB","colab_type":"text"},"source":["In order to apply embeddings for out categorical variables. We need to first determine which features are categorical variable and which are not. We need to ensure we caputure all possible cases of the categorical features. \n","\n","Okay, then for each categorical variable we need to capture the cardinalty of the feature iteself."]},{"cell_type":"code","metadata":{"id":"qCbbzW_yCym1","colab_type":"code","colab":{}},"source":["# load the embedding category mappings \n","cat_mappings = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', low_memory=False)\n","\n","# Load the embedding model\n","embed_model = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model.h5\")\n","embed_model2 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model2.h5\")\n","embed_model_final = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model_low_mae.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeVmG5ufCkpc","colab_type":"code","colab":{}},"source":["#fetch out the trained embedding model the model\n","model_embeddings = None\n","model_embeddings = create_model(saved_model=embed_model_final)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_BAmSdlgEOSk","colab_type":"text"},"source":["#### Category Encoder"]},{"cell_type":"code","metadata":{"id":"S_XVdZmZsogj","colab_type":"code","outputId":"bed1bf5d-e0c7-4db8-eeb7-1d70322f5a9e","executionInfo":{"status":"ok","timestamp":1587472512795,"user_tz":-120,"elapsed":311869,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# load the encoder model\n","encoder_model_embed_85 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_encoder_85.h5\")\n","encoder_model_embed_43 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_encoder_43.h5\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vUIGFrcegUrm","colab_type":"code","colab":{}},"source":["# this function manages the numerical stuffs\n","# given a data (scaled), it will generate a new representation of that data\n","def categorical_pipeline(data_in=None, data_embed_in=None, model_embed=None, model_encoder=None, cat_type=0):\n","\n","  if cat_type == 1:\n","    # means one hotencoding version:\n","    data_out = pd.get_dummies(data_embed_in, drop_first=True)\n","    return data_out.values\n","    \n","  # create the requirment for the embedding model\n","  XTrain, yTrain = network_input_process(data_num=None, data_cols=data_embed_in[cat_features], y_output=data_t_embed.iloc[:,3])\n","\n","  # Embedding Prediction\n","  embed_data = model_embed.predict(XTrain)\n","\n","  # the encoder model as well\n","  encoder_data = model_encoder.predict(embed_data)\n","\n","  # that's all here\n","  return encoder_data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YEG2UNcTjX_N","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Rtz6b6UCTmV","colab_type":"code","colab":{}},"source":["def dnn_model(input_size):\n","  ### Create the deep leaning model\n","  input_layer1 = Input(shape=(input_size,), name=\"feature_input_layer\")\n","  #encoder_layer1 = Dense(units=85, name='encoder_layer1', activity_regularizer=regularizers.l1(10e-5))(input_encoder)\n","  dense_layer1 = Dense(units=300, name='dense_layer1' )(input_layer1)\n","  activation_1 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer1)\n","  #activation_1 = ReLU()(dense_layer1)\n","  droput_layer1 = Dropout(0.2)(activation_1)\n","\n","  dense_layer2 = Dense(units=300, name='dense_layer2' )(droput_layer1)\n","  #activation_2 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer2_Activation')(dense_layer2)\n","  activation_2 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer2)\n","  #activation_2 = ReLU()(dense_layer2)\n","  droput_layer2 = Dropout(0.2)(activation_2)\n","\n","  dense_layer3 = Dense(units=300, name='dense_layer3' )(droput_layer2)\n","  #activation_3 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer3_Activation')(dense_layer3)\n","  activation_3 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer3)  \n","  #activation_3 = ReLU()(dense_layer3)\n","  droput_layer3 = Dropout(0.2)(activation_3)\n","\n","  dense_layer4 = Dense(units=300, name='dense_layer4')(droput_layer3)\n","  activation_4 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer4)  \n","  #activation_4 = ReLU()(dense_layer4)\n","  droput_layer4 = Dropout(0.2)(activation_4)\n","\n","  dense_layer5 = Dense(units=300, name='dense_layer5' )(droput_layer4)\n","  #activation_3 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer3_Activation')(dense_layer3)\n","  activation_5 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer5)  \n","  #activation_5 = ReLU()(dense_layer5)\n","  droput_layer5 = Dropout(0.2)(activation_5)\n","\n","  dense_layer6 = Dense(units=300, name='dense_layer6')(droput_layer5)\n","  activation_6 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer6)  \n","  #activation_6 = ReLU()(dense_layer6)\n","  droput_layer6 = Dropout(0.2)(activation_6)\n","\n","  # output layer\n","  dense_layer7 = Dense(units=1, name='output_layer')(droput_layer6)\n","\n","  # create the AE model\n","  rul_model = Model(inputs=input_layer1, outputs=dense_layer7, name=\"RUL_Prediction_Model\")\n","\n","  # compile the AE model\n","  rul_model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n","  return rul_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8gP9ec3KJCF6","colab_type":"code","colab":{}},"source":["def model_results(model_dev=None):\n","  predictions = model_dev.predict(data_test)\n","\n","  # plotting our prediction result\n","  test_predictions = pd.Series(predictions.reshape(data_test_output.shape[0],))\n","  pred_df = pd.DataFrame(data_test_output, columns=['True RUL'])\n","  pred_df = pd.concat([pred_df, test_predictions], axis=1)\n","  pred_df.columns = ['True RUL', 'Predicted RUL']\n","\n","  '''\n","  plt.figure()\n","  pred_df[:1000].plot(figsize=(20,10))\n","  plt.xlabel('Data Samples')\n","  plt.ylabel('RUL')\n","  plt.title('Random Forest Prediction on AE Embeddings')\n","  '''\n","\n","  # mean absolute error for our predictions\n","  mae_score = mean_absolute_error(pred_df['True RUL'], pred_df['Predicted RUL'])\n","\n","  # explained variance score\n","  ep_score = explained_variance_score(pred_df['True RUL'].values, pred_df['Predicted RUL'].values)\n","\n","  return mae_score, ep_score"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMFW3PWrTMB0","colab_type":"code","colab":{}},"source":["from tensorflow.keras.callbacks import ModelCheckpoint\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GdiyToTSLkgr","colab_type":"text"},"source":["# Training and Testing\n","Here we perform training and testing\n"]},{"cell_type":"markdown","metadata":{"id":"YtksyqRdkEXP","colab_type":"text"},"source":["#### Train"]},{"cell_type":"code","metadata":{"id":"hZsF3izTI-Qu","colab_type":"code","colab":{}},"source":["############# trial approach\n","input_encoder = Input(shape=(93,), name=\"encoder_input_layer\")\n","# create the first encoder layer\n","#encoder_layer1 = Dense(units=85, name='encoder_layer1', activity_regularizer=regularizers.l1(10e-5))(input_encoder)\n","encoder_layer1 = Dense(units=50, name='encoder_layer1', activation='sigmoid' )(input_encoder)\n","\n","#layer_1 = PReLU(alpha_initializer=Constant(value=0.3), name='enc_layer1_PReLU_Activation')(encoder_layer1)\n","#layer_1 = ReLU()(encoder_layer1)\n","\n","encoder_model = Model(inputs=input_encoder, outputs=encoder_layer1, name='Encoder_Model')\n","\n","# decoder 2nd decoder layer\n","decoder_layer1 = Dense(units=93, name='decoder_layer1', activation='relu')(encoder_layer1)\n","#layer_2 = PReLU(alpha_initializer=Constant(value=0.3), name='dec_layer1_PReLU_Activation')(decoder_layer1)\n","#layer_2 = ReLU()(decoder_layer1)\n","\n","#decoder = Model(inputs=input_decoder, outputs=decoder_layer1)\n","\n","# create the AE model\n","autoencoder = Model(inputs=input_encoder, outputs=decoder_layer1, name=\"AutoEncoder_Model\")\n","\n","# compile the AE model\n","autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy', 'mse'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tG_F1tmsJao0","colab_type":"code","outputId":"c391a9ae-058e-4e4d-b37a-619cf8d8fc68","executionInfo":{"status":"ok","timestamp":1587472517539,"user_tz":-120,"elapsed":316589,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## Creating a Combined AE Model\n","\n","data_t, data_v = validationSplit(data, split=0.2)\n","data_t.shape, data_v.shape\n","\n","data_t_embed, data_v_embed = validationSplit(data2, split=0.2)\n","\n","scaler = StandardScaler()\n","# perform scaling on the nummerical features\n","data_scaled = copy.copy(data_t)\n","data_scaled[num_features] = scaler.fit_transform(data_t[num_features])\n","\n","# AE combined\n","# training\n","data_num_test = numerical_pipeline(data_scaled, model1=encoder_vanila_50)\n","data_cat_train = categorical_pipeline(data_embed_in=data_t_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n","XTrain = np.concatenate((data_num_test, data_cat_train), axis=1)\n","\n","# testing\n","data_scaled = copy.copy(data_v)\n","data_scaled[num_features] = scaler.fit_transform(data_v[num_features])\n","data_cat_test = categorical_pipeline(data_embed_in=data_v_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n","data_num_test = numerical_pipeline(data_scaled, model1=encoder_vanila_50)\n","\n","XTest = np.concatenate((data_num_test, data_cat_test), axis=1)\n","XTest.shape, XTrain.shape                                     \n","                                      "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((3823, 93), (16131, 93))"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"9l_XGyoHJoOw","colab_type":"code","colab":{}},"source":["early_stop = EarlyStopping(monitor='val_mse', mode='max', verbose=1, patience=3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_1zIahyJmXr","colab_type":"code","outputId":"2ef77eab-7d05-4768-ef34-4a59fe9b4864","executionInfo":{"status":"ok","timestamp":1587472517541,"user_tz":-120,"elapsed":316581,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["autoencoder.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"AutoEncoder_Model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_input_layer (InputLa [(None, 93)]              0         \n","_________________________________________________________________\n","encoder_layer1 (Dense)       (None, 50)                4700      \n","_________________________________________________________________\n","decoder_layer1 (Dense)       (None, 93)                4743      \n","=================================================================\n","Total params: 9,443\n","Trainable params: 9,443\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wtIvvYdPJp-C","colab_type":"code","colab":{}},"source":["#autoencoder.fit(XTrain, XTrain, batch_size=128, epochs=100, validation_data=(XTest, XTest), workers=-1, callbacks=[early_stop])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J69KVxK5K2mr","colab_type":"code","outputId":"27acbc53-a6f1-4bf2-f3dd-96c3e21d5bf6","executionInfo":{"status":"ok","timestamp":1587472517542,"user_tz":-120,"elapsed":316572,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["encoder_model.predict(XTest)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.17388749, 0.20180562, 0.20002922, ..., 0.68169343, 0.43895572,\n","        0.08578089],\n","       [0.22877666, 0.18953484, 0.4561704 , ..., 0.6759254 , 0.2162736 ,\n","        0.12426397],\n","       [0.23881426, 0.17079046, 0.44612014, ..., 0.73192155, 0.22858322,\n","        0.14478421],\n","       ...,\n","       [0.3613472 , 0.37061346, 0.5495904 , ..., 0.44793692, 0.07125777,\n","        0.6242486 ],\n","       [0.7013226 , 0.5131718 , 0.8408166 , ..., 0.5519571 , 0.24835128,\n","        0.16672045],\n","       [0.35423136, 0.35495877, 0.270723  , ..., 0.63434464, 0.25195304,\n","        0.19204411]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"iBKESS-IO-j2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DpcrO9dgvkuz","colab_type":"code","outputId":"f8b4cfa6-c1de-48cd-bc11-93b5057bac29","executionInfo":{"status":"error","timestamp":1587472779130,"user_tz":-120,"elapsed":1272,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":282}},"source":["model_embeddings.get_layer(name='Num_Input_layer')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-4c5aecc890c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Num_Input_layer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No such layer: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No such layer: Num_Input_layer"]}]},{"cell_type":"code","metadata":{"id":"spssX-tQwGPI","colab_type":"code","outputId":"a2466b8f-70b2-4516-cbce-b96b3597fa7c","executionInfo":{"status":"ok","timestamp":1587472791232,"user_tz":-120,"elapsed":998,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model_embeddings.layers"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff569143e48>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff5691d9208>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55f51b128>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55f1059e8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55f55cbe0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55fea0668>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55fecfef0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec5fac8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec69518>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec79d68>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebfc978>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec083c8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec0ed68>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec1f828>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ec2d278>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebbcc88>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebc66d8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebd3f60>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebdbb38>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebe9588>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebf6dd8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb7e9e8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb8c438>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ecddef0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eba4438>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebb0d30>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ebb6940>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb42390>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb4ada0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb597f0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb67240>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb70ba8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eaff6a0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb0df28>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb14b00>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb21550>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb32da0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eb3a9b0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eac9400>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ead3da0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eadf860>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eaee2b0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eaf6cc0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea86710>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea94f98>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea9eb70>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eaaa5c0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55eab8e48>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea44a20>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea51470>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea59e10>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea688d0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea78320>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea00d30>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea0d780>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea1c128>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea25be0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55ea35630>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9c3eb8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9cba90>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9dc4e0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9e8d30>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9f0940>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e980390>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e989da0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9987f0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9a5240>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e9a9ba8>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e93d6a0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e94bf28>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e952b00>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e95f550>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e96eda0>,\n"," <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7ff55e97a9b0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec57a90>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55fa728d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55f552550>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55f0a81d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec32a58>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55feca4a8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55fecf438>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec6a908>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec74358>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec792e8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebff7b8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec0e208>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec16198>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ec24668>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebbc0b8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebc3048>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebcd518>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebd34a8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebe19b0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebef3c8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebf6358>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb84828>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55f196ba8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb9b898>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eba7320>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ebb02b0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb3d780>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb4a1d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb50160>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb62630>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb70080>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb78080>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb074e0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb0d470>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb1b940>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb2b390>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eb32320>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eac17f0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ead3240>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ead81d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eae96a0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eaf60f0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea7f080>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea8d550>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea944e0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eaa49e8>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eab3400>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55eab8390>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea4b860>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea592b0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea5f240>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea71710>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea00160>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea080f0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea155c0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea1cb38>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea2f0f0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea3a470>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9c3400>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9d48d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55ea42320>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9e82b0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9f5780>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9891d0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e98e160>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9a0630>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9a9080>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9b4080>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e9454e0>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e94b470>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e95a940>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e969390>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e96e320>,\n"," <tensorflow.python.keras.layers.embeddings.Embedding at 0x7ff55e8ff7f0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec57f28>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff56912c208>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55f5526a0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55f55c080>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55fea0a58>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55feca5f8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec5feb8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec6ae48>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec74e80>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebfcd68>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebff908>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec0ed30>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec1fc18>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ec247b8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebbc128>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebc6ac8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebcd668>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebdbf28>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebe9978>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebefef0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb7edd8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb84978>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ecddf98>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb9be80>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eba7e48>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ebb6d30>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb3d8d0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb4a240>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb59be0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb62780>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb700f0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eaffa90>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb07630>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb14ef0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb21908>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb2beb8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eb3ada0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eac1940>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ead3d68>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eadfc50>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eae97f0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eaf6160>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea86b00>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea8d6a0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea9ef60>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eaaa9b0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55eab3fd0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea44e10>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea4b9b0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea59dd8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea68cc0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea71860>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea001d0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea0db70>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea15710>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea25fd0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea35a20>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea3a5c0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9cbe80>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9d4b38>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55ea42e48>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9f0d30>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9f58d0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e989240>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e998be0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9a0780>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e9a90f0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e93da90>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e945630>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e952ef0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e95f908>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e969eb8>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e97ada0>,\n"," <tensorflow.python.keras.layers.core.Reshape at 0x7ff55e8ff940>,\n"," <tensorflow.python.keras.layers.merge.Concatenate at 0x7ff55e903630>]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"RFb9guEd6eX_","colab_type":"code","colab":{}},"source":["model_embeddings.predict(data_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HAsTf9DPEY7M","colab_type":"code","colab":{}},"source":["chassis_group = data['T_CHASSIS'].values\n","\n","cv_splits = crossValidationSplit(kfolds=5, split=0.2)\n","\n","# filepath for saving best models\n","filepath=\"rul_weights_best.hdf5\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nPBL2lA4Elpq","colab_type":"code","outputId":"4bd4786c-4edb-47fb-fcc4-7ca9fe7c96b4","executionInfo":{"status":"ok","timestamp":1587472600509,"user_tz":-120,"elapsed":399530,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"source":["# Performing 5 Fold cross validation\n","cv_splits = crossValidationSplit(kfolds=5, split=0.2)\n","\n","cv_count = 0\n","mae_score_lists = []\n","ep_score_lists = []\n","for train_idx, test_idx in cv_splits.split(data, groups=chassis_group):\n","  cv_count = cv_count + 1\n","  print (\"Kfold Cross Validation Count - \", cv_count)\n","  data_t = data.iloc[train_idx, :].reset_index()\n","  data_v = data.iloc[test_idx, :].reset_index()\n","\n","  ####\n","  data_t_embed = data2.iloc[train_idx, :].reset_index()\n","  data_v_embed = data2.iloc[test_idx, :].reset_index()  \n","  \n","  # define the scaler\n","  scaler = StandardScaler()\n","\n","  #### Training Data\n","  # perform scaling on the nummerical features\n","  data_scaled = copy.copy(data_t)\n","  data_scaled[num_features] = scaler.fit_transform(data_t[num_features])\n","\n","  # fetch out the numerical representation\n","  #data_num_train = numerical_pipeline(data_scaled, model1=encoder_model)\n","  data_num_train = data_scaled[num_features].values\n","  # fetch out the categorical representation\n","  #data_cat_train = categorical_pipeline(data_embed_in=data_t_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n","  # one hot encoding version of the categorical features\n","  \n","  # one hot encoding version of the categorical features\n","  data_one_hot = pd.get_dummies(data[cat_features], drop_first=True)\n","\n","  # split data to train and test - For the embedding informtion\n","  data_t_hot = data_one_hot.iloc[train_idx, :]\n","  data_v_hot = data_one_hot.iloc[test_idx, :]\n","  data_cat_train = data_t_hot.values\n","  \n","   # merge together\n","  data_train = np.concatenate((data_num_train, data_cat_train), axis=1)\n","  data_train = data_num_train\n","  data_train_output = data_t['RUL'].values\n","\n","  print ('Training data - ', data_num_train.shape, data_cat_train.shape, data_train.shape)\n","\n","  #### Testing stage\n","  data_scaled = copy.copy(data_v)\n","  data_scaled[num_features] = scaler.transform(data_v[num_features])\n","\n","  # fetch out the numerical representation\n","  #data_num_test = numerical_pipeline(data_scaled, model1=encoder_model)\n","  data_num_test = data_scaled[num_features].values\n","\n","  # fetch out the categorical representation\n","  #data_cat_test = categorical_pipeline(data_embed_in=data_v_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n","  # one hot encoding version of the categorical features\n","  data_cat_test = data_v_hot.values\n","\n","  # merge together\n","  data_test = np.concatenate((data_num_test, data_cat_test), axis=1)\n","  data_test = data_num_test\n","  data_test_output = data_v['RUL'].values\n","  print ('Testing data - ', data_num_test.shape, data_cat_test.shape, data_test.shape)\n","\n","  ### Evaluation\n","  ''' Random Forest model\n","  regr = RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=100, criterion='mae', max_depth=100, verbose=1)\n","  regr.fit(data_train, data_train_output)\n","  '''\n","  # DNN model\n","  # create a checkpoint to save only the best model\n","  checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=0, save_best_only=True, mode='min')\n","  rul_model = dnn_model(data_train.shape[1])\n","  early_stop = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=5)\n","  rul_model.fit(x=data_train, y=data_train_output, epochs=100, batch_size=128, verbose=0, validation_data=(data_test, data_test_output), workers=-1, callbacks=[early_stop, checkpoint])\n","\n","  # results\n","  # evaluate using the best model\n","  # load the best weights\n","  rul_model.load_weights(filepath)\n","  rul_model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n","  mae_score, ep_score = model_results(model_dev=rul_model)\n","\n","  print (\"Model Score for Fold - \", cv_count)\n","  print (\"MAE: \", mae_score)\n","  print (\"Explained_Variance: \", ep_score)\n","  # add score to list\n","  mae_score_lists.append(mae_score)\n","  ep_score_lists.append(ep_score)\n","\n","  print (\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Kfold Cross Validation Count -  1\n","Training data -  (16194, 362) (16194, 200) (16194, 362)\n","Testing data -  (3760, 362) (3760, 200) (3760, 362)\n","Epoch 00006: early stopping\n","Model Score for Fold -  1\n","MAE:  173.93519820976763\n","Explained_Variance:  0.2607347691907467\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","Kfold Cross Validation Count -  2\n","Training data -  (15972, 362) (15972, 200) (15972, 362)\n","Testing data -  (3982, 362) (3982, 200) (3982, 362)\n","Epoch 00008: early stopping\n","Model Score for Fold -  2\n","MAE:  214.67879514081102\n","Explained_Variance:  -0.6113207723473639\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","Kfold Cross Validation Count -  3\n","Training data -  (16063, 362) (16063, 200) (16063, 362)\n","Testing data -  (3891, 362) (3891, 200) (3891, 362)\n","Epoch 00014: early stopping\n","Model Score for Fold -  3\n","MAE:  203.3956005806728\n","Explained_Variance:  -0.09897744567473565\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","Kfold Cross Validation Count -  4\n","Training data -  (16005, 362) (16005, 200) (16005, 362)\n","Testing data -  (3949, 362) (3949, 200) (3949, 362)\n","Epoch 00006: early stopping\n","Model Score for Fold -  4\n","MAE:  206.2383447161926\n","Explained_Variance:  -1.275273599131273\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","Kfold Cross Validation Count -  5\n","Training data -  (15969, 362) (15969, 200) (15969, 362)\n","Testing data -  (3985, 362) (3985, 200) (3985, 362)\n","Epoch 00006: early stopping\n","Model Score for Fold -  5\n","MAE:  173.99229666280328\n","Explained_Variance:  0.3025199836744288\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RFkIs3hPJzsJ","colab_type":"code","outputId":"bb7e3b12-27dd-4dcc-d181-f7210f9dba8f","executionInfo":{"status":"ok","timestamp":1587472600511,"user_tz":-120,"elapsed":399524,"user":{"displayName":"Ayo Ayibiowu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdfnJKPQbuwXCnDR_yv_rHBEhflS9Dtr8-8tEq3w=s64","userId":"01321082632502612911"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["########\n","print (\"#######################################\")\n","print (\"Kfold Cross Validation is Complete. Total Folds - \", cv_count)\n","print (\"MAE Average Score is : \", np.mean(mae_score_lists))\n","print (\"MAE List - \", mae_score_lists)\n","print (\"Explained Variance Average Score is : \", np.mean(ep_score_lists))\n","print (\"#######################################\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["#######################################\n","Kfold Cross Validation is Complete. Total Folds -  5\n","MAE Average Score is :  194.44804706204945\n","MAE List -  [173.93519820976763, 214.67879514081102, 203.3956005806728, 206.2383447161926, 173.99229666280328]\n","Explained Variance Average Score is :  -0.28446341285763943\n","#######################################\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6qi_OJXbNoo7","colab_type":"text"},"source":["With my own 50 AE, I am getting:\n","#######################################\n","Kfold Cross Validation is Complete. Total Folds -  5\n","MAE Average Score is :  166.98904024892346\n","MAE List -  [164.91209582825925, 184.3998512223041, 170.17526329773858, 161.18372303771199, 154.27426785860337]\n","Explained Variance Average Score is :  0.23608884206782074\n","#######################################"]},{"cell_type":"markdown","metadata":{"id":"q2QooK-ldQcI","colab_type":"text"},"source":["[152.79084035589736, 164.67501267699487, 145.9169874509911, 162.08565716949948, 143.87037558226544]\n","153.867774647129\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"LLxCq8euZDd-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eYNa0V15ZD9u","colab_type":"text"},"source":["MAE List -  [143.6790844776529, 152.57317720448654, 150.20180196303752, 148.66132092107608, 141.44549391159]\n"]},{"cell_type":"markdown","metadata":{"id":"5vkxNOP8Q8J4","colab_type":"text"},"source":["Kfold Cross Validation is Complete. Total Folds -  5\n","MAE Average Score is :  166.67422428659142\n","MAE List -  [162.3936416305126, 175.5590205381768, 153.3340330050372, 177.5236260111527, 164.56080024807784]\n","Explained Variance Average Score is :  0.384136792505209\n","With early stopping of 2"]},{"cell_type":"markdown","metadata":{"id":"27bn43f_nneL","colab_type":"text"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"Qq-J_V4M0CpN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPxJnxD44vHk","colab_type":"text"},"source":["165.8035277806431 - 6 layers using 300 units"]}]}