{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extraction_Pipeline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNiJGTptJztBe1TA/23kKZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thehapyone/Thesis_Project/blob/master/Extraction_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yylXv6VzNeMR",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering & Data Cleaning and Augumentation Complete Pipeline\n",
        "This code performs different levels of data augumentation on the provided data automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy1BUNS6NtnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy as copy\n",
        "# using interativeimputer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlHnR6mnNztO",
        "colab_type": "text"
      },
      "source": [
        "## Duplicates Removal Pipeline\n",
        "This stage removes duplicates features from the input data.\n",
        "Input - A Pandas dataframe with n columns\n",
        "Output - A Pandas Dataframe with n-d columns. Where d is the no of duplicates columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0WYsomVOO5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function returns the duplicate columns\n",
        "def getDuplicateColumns(df):\n",
        "    '''\n",
        "    Get a list of duplicate columns.\n",
        "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
        "    :param df: Dataframe object\n",
        "    :return: List of columns whose contents are duplicates.\n",
        "    '''\n",
        "    duplicateColumnNames = set()\n",
        "    # Iterate over all the columns in dataframe\n",
        "    for x in range(df.shape[1]):\n",
        "        # Select column at xth index.\n",
        "        col = df.iloc[:, x]\n",
        "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
        "        for y in range(x + 1, df.shape[1]):\n",
        "            # Select column at yth index.\n",
        "            otherCol = df.iloc[:, y]\n",
        "            # Check if two columns at x 7 y index are equal\n",
        "            if col.equals(otherCol):\n",
        "                duplicateColumnNames.add(df.columns.values[y])\n",
        " \n",
        "    return list(duplicateColumnNames)\n",
        "\n",
        "# function removes duplicate features from the data\n",
        "def removeDuplicates(raw_data, debug=False):\n",
        "  # fetch out the duplicates columns - It will take sometime: Quadratic time complexity\n",
        "  dup_columns = getDuplicateColumns(raw_data)\n",
        "  if debug == True:\n",
        "    print ('Duplicate columns are: ')\n",
        "    print (dup_columns)\n",
        "\n",
        "  # now we drop all the duplicate columns\n",
        "  output = raw_data.drop(labels=dup_columns, axis=1)\n",
        "\n",
        "  return output\n",
        "\n",
        "# calling this is easy as\n",
        "# my_data = removeDuplicates(raw_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH3Hu4oqQa4T",
        "colab_type": "text"
      },
      "source": [
        "## Remove Zero Variance Pipeline\n",
        "This removes all feature with no changes to their elements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wErEqNjQoAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def removeZeroVariance(raw_data, debug=False):\n",
        "  # finding column data with almost zero standard deviation. \n",
        "  std_data = raw_data.describe().transpose().loc[:,'std']\n",
        "\n",
        "  # this columns have zero standard deviation\n",
        "  zero_std_columns = std_data[std_data == 0]\n",
        "\n",
        "  if debug == True:\n",
        "    print (\"Features with Zero Variance\")\n",
        "    print (zero_std_columns)\n",
        "\n",
        "  # drop all features with zero variance\n",
        "  zero_std_columns_list = list(zero_std_columns.index)\n",
        "  output = raw_data.drop(labels=zero_std_columns_list, axis=1)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhmQgt6iRZda",
        "colab_type": "text"
      },
      "source": [
        "## Drop Redundant Features\n",
        "Here we can remove features we think isn't going to be helpful.\n",
        "The function will support user inputted features as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY2NreHoSKr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropRedundantFeatures(raw_data, to_remove=[], debug=False):\n",
        "  # we have define a set of features to be removed normally\n",
        "  if len(to_remove) == 0:\n",
        "    to_remove = ['Index', 'DELIVERY_DATE', 'LAST_RUN', 'LAST_RUN.1']\n",
        "  \n",
        "  # now we drop the columns\n",
        "  output = raw_data.drop(labels=to_remove, axis=1)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAsY0y6TPit",
        "colab_type": "text"
      },
      "source": [
        "## Date Information Extractor\n",
        "Here, we will extact information about the day, month, and year from the BiWeek_Send_Date feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biMymsghTjRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dateExtractor(raw_data, debug=True):\n",
        "\n",
        "  output = copy(raw_data)\n",
        "  # function extracts out the day, month, and year information from the 'BiWeek Send data' information.\n",
        "  output['BIWEEK_SEND_DATE'] = pd.to_datetime(raw_data['BIWEEK_SEND_DATE'], infer_datetime_format=True)\n",
        "\n",
        "  # Now we will extract out the day, month, and year information from the 'BiWeek Send data' information.\n",
        "  day_data = output['BIWEEK_SEND_DATE'].dt.day\n",
        "  month_data = output['BIWEEK_SEND_DATE'].dt.month\n",
        "  year_data = output['BIWEEK_SEND_DATE'].dt.year\n",
        "\n",
        "  # insert into the dataframe\n",
        "  # Re-order the arrangments of the columns. Goal is to move 'Day', 'Month', and 'year' to the front\n",
        "  output.insert(1, 'DAY', day_data)\n",
        "  output.insert(2, 'MONTH', month_data)\n",
        "  output.insert(3, 'YEAR', year_data)\n",
        "\n",
        "  # now we will drop the BIWEEK_SEND_DATE\n",
        "  output = output.drop(labels=['BIWEEK_SEND_DATE'], axis=1)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0s7WtqHWss8",
        "colab_type": "text"
      },
      "source": [
        "## Filling Missing Values Pipeline\n",
        "In this stage, I attempt to fill missing values in the data\n",
        "\n",
        "\n",
        "*   The following features will be cleaned and updated - 'LX_PSC_P1FUO_NUMBER_OF_STARTER_ACTIVATION_D', 'LX_PSC_P1FUN_NUMBER_OF_ENGINE_STARTS_DURING', 'LX_PST_P1B3A_332_BCLVEHICLEMODECRANKINGTIME_LOG'\n",
        "*   The following features will be cleaned and updated - LX_PSC_P1ASU_MAIN_LOG_DRIVE_FUEL and LX_PSC_P1ATK_MAIN_LOG_ECONOMICAL_FUEL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42G2rUABXme7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stageFilling(raw_data, impute_cols=[], debug=False):\n",
        "  output = copy.copy(raw_data)\n",
        "  # create the imputer\n",
        "  imp = IterativeImputer(max_iter=10, random_state=42)\n",
        "  # stage 1 of the filling pipeline\n",
        "  missing_cols_1 = impute_cols\n",
        "  # create the dataFrame\n",
        "  data_stage1 = output[missing_cols_1]\n",
        "\n",
        "  # see the current state\n",
        "  if debug == True:\n",
        "    print (\"Before imputation\")\n",
        "    print(data_stage1.isnull().sum().sort_values(ascending=False))\n",
        "  \n",
        "  # perform imputation on the missing values\n",
        "  result = pd.DataFrame(imp.fit_transform(data_stage1.values), columns=data_stage1.columns)\n",
        "\n",
        "  # see the resulting state\n",
        "  if debug == True:\n",
        "    print (\"After imputation\")\n",
        "    print(data_stage1.isnull().sum().sort_values(ascending=False))\n",
        "\n",
        "  # now update the dataframe with the new changes\n",
        "  output.update(results)\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def fillMissingValues(raw_data, debug=False):\n",
        "  # We define the columns we want to perform imputation on\n",
        "  missing_cols = ['LX_PSC_P1FUO_NUMBER_OF_STARTER_ACTIVATION_D', 'LX_PSC_P1FUN_NUMBER_OF_ENGINE_STARTS_DURING', 'LX_PST_P1B3A_332_BCLVEHICLEMODECRANKINGTIME_LOG']\n",
        "  results1 = stageFilling(raw_data, impute_cols=missing_cols, debug=debug)\n",
        "\n",
        "  # for stage 2\n",
        "  # We define the columns we want to perform imputation on\n",
        "  missing_cols = ['LX_PSC_P1ASU_MAIN_LOG_DRIVE_FUEL', 'LX_PSC_P1BBY_TOTAL_FUEL_CONSUMPTION', 'LX_PSC_P1ATO_MAIN_LOG_TOTAL_ENERGY', 'LX_PSC_P1ATK_MAIN_LOG_ECONOMICAL_FUEL']\n",
        "  results2 = stageFilling(results1, impute_cols=missing_cols, debug=debug)\n",
        "\n",
        "  # other high level imputation can be perfromed here as well\n",
        "  \n",
        "  return results2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}