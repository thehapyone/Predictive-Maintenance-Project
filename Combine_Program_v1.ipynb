{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Combine_Program_v1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thehapyone/Thesis_Project/blob/master/Combine_Program_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBH4RjYX1OGw",
        "colab_type": "text"
      },
      "source": [
        "# Main Program for Numerical and Categorical Feature\n",
        "This program combines the models built for both the numerical and categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kS2iuLi1tCV",
        "colab_type": "text"
      },
      "source": [
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-connect-button\").shadowRoot.getElementById('connect').click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAQD2JtYkD2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 5\n",
        "\n",
        "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed_value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7mCtb7J1TX8",
        "colab_type": "code",
        "outputId": "5d701dbe-fdad-483f-9402-29dce4f37253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1JPb4v074V",
        "colab_type": "code",
        "outputId": "6af289db-78ff-476b-fe0d-e55b707d8add",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy as copy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI-9fGnufnoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use tensorflow version 2\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "# load the encoder model to be used\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Reshape, Concatenate, Dense, Flatten, Dropout\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZslDysrzghEL",
        "colab_type": "code",
        "outputId": "0f3c6c8b-8991-4869-85cc-1cdc8ec21674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.layers import PReLU, ReLU, BatchNormalization, ELU\n",
        "from keras.initializers import Constant\n",
        "from keras import regularizers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsk9Eigsjn5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, explained_variance_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oD7nqTYf6a2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "# for cross validation, we can use Group KFold spliting - GroupShuffleSplit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRGnXWAW-rXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Thesis/Collab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VEwTzseoK3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the Extraction pipeline\n",
        "# first stage of the pipeline\n",
        "from extraction_pipeline import *\n",
        "from category_pipeline import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjSOi7PZ1VFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using the latest updated file\n",
        "#data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v3_Sorted_Raw_Db_with_RUL.csv\", sep=',', low_memory=False)\n",
        "#ata = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v4_Sorted_Raw_Db_with_RUL_imputed.csv\", sep=',', low_memory=False)\n",
        "#data = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/sorted_data_new_full.csv\", sep=',', low_memory=False)\n",
        "#data = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/sorted_data_new.csv\", sep=',', low_memory=False)\n",
        "data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v5_Sorted_Raw_Db_with_RUL_imputed.csv\", sep=',', low_memory=False)\n",
        "data2 = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/v2_sorted_data_new.csv\", sep=',', low_memory=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14TT33eD3Qb",
        "colab_type": "text"
      },
      "source": [
        "## Category Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsHVm38hD5V_",
        "colab_type": "code",
        "outputId": "709f890c-d402-4848-db08-ab5e3f82beb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Date transformation\n",
        "data = extraTrasfromToCategory(data, debug=True)\n",
        "\n",
        "'''\n",
        "# Zero Variance removal\n",
        "data = removeZeroCategory(data)\n",
        "print ('After all removal pipeline - ',data.shape)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dtypes are: \n",
            "DAY                                  object\n",
            "MONTH                                object\n",
            "YEAR                                 object\n",
            "VFE_0005_VEHICLE_OPERATION_DIGIT1    object\n",
            "VFE_0006_VEHICLE_OPERATION_DIGIT2    object\n",
            "VFE_0008_HAS_PTO                     object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Zero Variance removal\\ndata = removeZeroCategory(data)\\nprint ('After all removal pipeline - ',data.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF2B3mwOqYOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load this information from the save csv file\n",
        "cat_features_df = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_embedding_details.csv\", sep=',', low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OwJu5BMlcU",
        "colab_type": "code",
        "outputId": "dcb61c7e-b9d4-4cbc-9409-3b9e5f15ddb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19954, 447)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCRLRt0TouM",
        "colab_type": "text"
      },
      "source": [
        "(336677, 449)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B7KXV8TMb4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load feature names from db\n",
        "feature_names_cat = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_cat.csv\", sep=',', low_memory=False)\n",
        "feature_names_num = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_num.csv\", sep=',', low_memory=False)\n",
        "# fetch the categorical and numerical features\n",
        "cat_features = feature_names_cat['Categorical'].values\n",
        "num_features = feature_names_num['Numerical'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlkJq-7lPpRw",
        "colab_type": "code",
        "outputId": "75e7c0e0-2500-4812-a034-c45f9a7226dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_features.shape, cat_features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((362,), (74,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Y43CX2QWmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for assisting in spliting the data\n",
        "def validationSplit(data_in, split=0.2, toShuffle=False):\n",
        "  # here we will split the data\n",
        "  # splitting by chassis\n",
        "  chassis_data = data_in['T_CHASSIS'].unique()\n",
        "  # splitting the chassis data\n",
        "  c_train, c_val = train_test_split(chassis_data, test_size=split, random_state=42, shuffle=toShuffle)\n",
        "  # extracting out the training and testing\n",
        "  train_data = data_in[data_in['T_CHASSIS'].isin(c_train)].reset_index(drop=True)\n",
        "  test_data = data_in[data_in['T_CHASSIS'].isin(c_val)].reset_index(drop=True)\n",
        "  return train_data, test_data\n",
        "\n",
        "# Function for assisting in spliting the data\n",
        "def crossValidationSplit(kfolds=5, split=0.2, toShuffle=False, ):\n",
        "  # here we will split the data based on their chassis grouping\n",
        "  ## generating the Group Parameters\n",
        "  c_split = GroupShuffleSplit(n_splits=kfolds, test_size=0.2, random_state=42)\n",
        "  '''\n",
        "  # splitting the data\n",
        "  for train_idx, test_idx in c_split.split(data.values, groups=chassis_group):\n",
        "    print(\"TRAIN:\", data.values[train_idx].shape, \"TEST:\", data.values[test_idx].shape)\n",
        "  '''\n",
        "  return c_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lzJJLj29fn6",
        "colab_type": "text"
      },
      "source": [
        "### Numerical Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09AZMOXA7_ra",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6XyCKm8ymo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we will load the learned pipeline selected features\n",
        "# load feature names from db\n",
        "feature_names_learning = pd.read_csv(\"/content/drive/My Drive/Thesis/data/Learning Pipeline/Best_Features_from_Feature_Selection.csv\", sep=',', low_memory=False)\n",
        "# fetch the learned features\n",
        "learned_features = feature_names_learning['Best_100_Feature Names'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASiNt8Sf9ZN0",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSq7A1eMHAji",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6vkc4d-h7VN",
        "colab_type": "code",
        "outputId": "2bd6991d-0a02-4a73-d85c-d5e4bc7b1f8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "\n",
        "def create_num_models():\n",
        "  # load the embedding model\n",
        "  # identical to the previous one\n",
        "  vanila_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Vanilla_AE_20.h5\")\n",
        "  denoising_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Denoising_AE.h5\")\n",
        "\n",
        "  # extract out the vanila model\n",
        "  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\n",
        "  output_layer = vanila_num_model.get_layer(name='bottleneck_layer')(en_input_layer)\n",
        "  # create the encoder model for the vanila model\n",
        "  encoder_vanila = Model(inputs=en_input_layer, outputs=output_layer, name='20_features_Vanila_Encoder')\n",
        "\n",
        "  # extract out the denoising model\n",
        "  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\n",
        "  enc_layer_1 = denoising_num_model.get_layer(name='enc_layer_1')(en_input_layer)\n",
        "  enc_layer_2 = denoising_num_model.get_layer(name='enc_layer_2')(enc_layer_1)\n",
        "  enc_layer_3 = denoising_num_model.get_layer(name='enc_layer_3')(enc_layer_2)\n",
        "\n",
        "  #output_layer = vanila_num_model.get_layer(name='bottleneck_layer')(en_input_layer)\n",
        "  # create the encoder model for the denosing model\n",
        "  encoder_denosing = Model(inputs=en_input_layer, outputs=enc_layer_3, name='25_features_denosing_Encoder')\n",
        "\n",
        "  return encoder_vanila, encoder_denosing\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\ndef create_num_models():\\n  # load the embedding model\\n  # identical to the previous one\\n  vanila_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Vanilla_AE_20.h5\")\\n  denoising_num_model = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Combine_Num_Cat_Pipelines/Denoising_AE.h5\")\\n\\n  # extract out the vanila model\\n  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\\n  output_layer = vanila_num_model.get_layer(name=\\'bottleneck_layer\\')(en_input_layer)\\n  # create the encoder model for the vanila model\\n  encoder_vanila = Model(inputs=en_input_layer, outputs=output_layer, name=\\'20_features_Vanila_Encoder\\')\\n\\n  # extract out the denoising model\\n  en_input_layer = Input(shape=(100,), name=\"en_layer_100\")\\n  enc_layer_1 = denoising_num_model.get_layer(name=\\'enc_layer_1\\')(en_input_layer)\\n  enc_layer_2 = denoising_num_model.get_layer(name=\\'enc_layer_2\\')(enc_layer_1)\\n  enc_layer_3 = denoising_num_model.get_layer(name=\\'enc_layer_3\\')(enc_layer_2)\\n\\n  #output_layer = vanila_num_model.get_layer(name=\\'bottleneck_layer\\')(en_input_layer)\\n  # create the encoder model for the denosing model\\n  encoder_denosing = Model(inputs=en_input_layer, outputs=enc_layer_3, name=\\'25_features_denosing_Encoder\\')\\n\\n  return encoder_vanila, encoder_denosing\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78egEozwhuJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the numerical encoding model\n",
        "\n",
        "#encoder_vanila, encoder_denosing = create_num_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjtjGm6zsNn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_vanila_50 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_50.h5\",compile= False)\n",
        "encoder_vanila_40 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_40.h5\",compile= False)\n",
        "encoder_vanila_30 = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Vanilla_Encoder_30.h5\",compile= False)\n",
        "\n",
        "encoder_denosing = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Denoising_Encoder.h5\",compile= False)\n",
        "encoder_deep = load_model(\"/content/drive/My Drive/Thesis/Collab Notebooks/Numerical_Encoder_models/Deep_Encoder.h5\",compile= False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP7HVJIkehjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function manages the numerical stuffs\n",
        "# given a data (scaled), it will generate a new representation of that data\n",
        "def numerical_pipeline(data_in=None, model1=None, model2=None):\n",
        "  # first extract out the learning pipeline\n",
        "  data_num = data_in[learned_features].values\n",
        "\n",
        "  # the encoder model as well - first model\n",
        "  encoder_1_data = model1.predict(data_num)\n",
        "  # encoder_2_data = model2.predict(data_num)\n",
        "\n",
        "  # merge them together\n",
        "  # encoder_num = np.concatenate((encoder_1_data, encoder_2_data), axis=1)\n",
        "  \n",
        "  # that's all here\n",
        "  return encoder_1_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rMSkVNE_gr8",
        "colab_type": "text"
      },
      "source": [
        "### Category Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twp6hyyLNLHg",
        "colab_type": "text"
      },
      "source": [
        "In summary, we have a total of 74 categories. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4FSV3zNC3Q5",
        "colab_type": "text"
      },
      "source": [
        "#### Extras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQrVg012oYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Goal\n",
        "'''\n",
        "Write a function, when given a categorical variable, it will generate the vector representation for that variable.\n",
        "In order to achieve that, we first need to map very categorical variable to some numerical index value. This value \n",
        "has to be unique and not random. \n",
        "'''\n",
        "\n",
        "def transfromForEmbed(data_cat, cat_feature_name='', cat_mapping=''):\n",
        "  # find the cat_index for all the features\n",
        "  def findCatIndex(x):\n",
        "    return int(cat_mapping[cat_mapping[cat_feature_name] == x][cat_feature_name+'_index'].values[0])\n",
        "  output = data_cat[cat_feature_name].apply(findCatIndex) \n",
        "  return output\n",
        "\n",
        "def embedTransform(model=None, variable='', cat_feature_name='', cat_mapping='', return_type=1):\n",
        "  # Given a categorical variable, produce the embedded vector\n",
        "  # Find the index of the categorical variable in the cat_mapping dataframe\n",
        "  cat_index = int(cat_mapping[cat_mapping[cat_feature_name] == variable][cat_feature_name+'_index'].values[0])\n",
        "  if return_type == 0:\n",
        "    return cat_index\n",
        "  # fetch the embeddings weights\n",
        "  pre_embedding = \"Embedding_layer_\"\n",
        "  embed_layer = model.get_layer(name=pre_embedding+cat_feature_name)\n",
        "  embedd_vector = embed_layer.get_weights()[0][cat_index]\n",
        "  return embedd_vector\n",
        "\n",
        "def createCatMapping(data_cat):\n",
        "  # this function will create a mapping dataframe\n",
        "  cat_size = data_cat.shape[1]\n",
        "  feature_names = data_cat.columns\n",
        "\n",
        "  cat_mapping = pd.DataFrame()\n",
        "\n",
        "  col_mapping_list = list()\n",
        "  col_mapping_cols = list()\n",
        "  # try for all dataframes\n",
        "  for i in range(cat_size):\n",
        "    col_indexs = list(pd.factorize(data_cat.iloc[:, i].unique())[0])\n",
        "    # save to list\n",
        "    col_mapping_list.append(list(data_cat.iloc[:, i].unique()))\n",
        "    col_mapping_list.append(col_indexs)\n",
        "    # save the column name as well\n",
        "    col_mapping_cols.append(feature_names[i])\n",
        "    col_mapping_cols.append(feature_names[i]+'_index')\n",
        "\n",
        "  # now we need to save the list to a dataframe\n",
        "  cat_mapping = pd.DataFrame(col_mapping_list)\n",
        "  # transpose it\n",
        "  cat_mapping = cat_mapping.transpose()\n",
        "  # update the columns names\n",
        "  cat_mapping.columns = col_mapping_cols\n",
        "\n",
        "  # save to the dataframe\n",
        "  cat_mapping.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', index=False)\n",
        "\n",
        "#createCatMapping(data[cat_features]) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZS29ptVFn6s",
        "outputId": "0c1a01e0-68dd-41a5-b1c0-fc07cb615cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1, embeddings_metadata=embeddings_meta, write_images=True)\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1,\n",
        "                                   write_images=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6JUt7abZXbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Creating the Keras Embeddings Mode\n",
        "\n",
        "def create_model(saved_model=None):\n",
        "  # IDs representing 1-hot encodings\n",
        "  # Need to create the input for all features\n",
        "  cat_embedding_input_layers = []\n",
        "\n",
        "  cat_reshape_layers = []\n",
        "  # Embeddings for the first cat feature\n",
        "  id_feature = 0\n",
        "  # create an array of embeddings layers\n",
        "  cat_embedding_layers = []\n",
        "\n",
        "  # interate through categorical varaibles\n",
        "  for id_feature in range(cat_features_df.shape[0]):\n",
        "    # creating the input layer for the embeddings\n",
        "    #input_layer = Input(shape=(1,), name=\"Input_layer_\"+cat_features_df.loc[id_feature, 'Feature'])\n",
        "    input_layer = Input(shape=(1,), name=\"Input_layer_\"+cat_features_df.loc[id_feature, 'Feature'])\n",
        "    cat_embedding_input_layers.append(input_layer)\n",
        "    # embedding size \n",
        "    layer_embedding_size = cat_features_df.loc[id_feature, 'Embedding_Size']\n",
        "    # create the embedding layers\n",
        "    embed_weights = saved_model.get_layer(name=\"Embedding_layer_\"+cat_features_df.loc[id_feature, 'Feature']).get_weights()\n",
        "    #embedded_layer.trainable = False\n",
        "    embedded_layer = Embedding(input_dim=cat_features_df.loc[id_feature, 'Cardinality'], output_dim=layer_embedding_size, \n",
        "                               name=\"Embedding_layer_\"+cat_features_df.loc[id_feature, 'Feature'], input_length = 1, trainable=False, weights=embed_weights)(input_layer)\n",
        "    cat_embedding_layers.append(embedded_layer)\n",
        "    # add a reshape of the embedding layers\n",
        "    reshape_layer = Reshape(target_shape=(layer_embedding_size,))(embedded_layer)\n",
        "    # appends the rehshape models together\n",
        "    cat_reshape_layers.append(reshape_layer)\n",
        "\n",
        "  ### Create A combined embedding layers only\n",
        "  combined_emb = Concatenate(axis=1, name='combined_embeddings')(cat_reshape_layers)\n",
        "  \n",
        "  ### the output of the combine embedding model is 169 vectors\n",
        "\n",
        "  ### creating a model based on the trained weights\n",
        "  model_emb = Model(inputs=cat_embedding_input_layers, outputs=combined_emb, name=\"Categorical_Embeddings\")\n",
        "\n",
        "  # compile the model - for a regression model\n",
        "  model_emb.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
        "  # compile the model - for a classification problem\n",
        "  # model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  return model_emb\n",
        "\n",
        "def train_model(model, x_train, y_train, x_test, y_test, epochs, batch_size, early_stop_callback=None, tensorboard_callback=None):\n",
        "  # fitting the model\n",
        "  #model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_data=(XTest, yTest), callbacks=[early_stop, tensorboard_callback])\n",
        "  model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[early_stop_callback, tensorboard_callback])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WFSz0kSuuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For each run, log an hparams summary with the hyperparameters and final accuracy:\n",
        "def run(my_model, x_train, y_train, epochs, batch_size, early_stop_callback, tensorboard_log):\n",
        "  model = None\n",
        "  model = create_model(my_model)\n",
        "  model_output = train_model(model, x_train, y_train, epochs, batch_size, early_stop_callback=early_stop_callback, tensorboard_callback=tensorboard_log)\n",
        "  # do something with the metrics\n",
        "  return model_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbuVc8kGWYqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUlRi7_CozT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transfromForEmbed(data_cat, cat_feature_name='', cat_mapping='', type=None):\n",
        "  # find the cat_index for all the features\n",
        "  def findCatIndex(x):\n",
        "    return int(cat_mapping[cat_mapping[cat_feature_name] == x][cat_feature_name+'_index'].values[0])\n",
        "  if type is not None:\n",
        "    output = findCatIndex(data_cat[cat_feature_name])\n",
        "    return np.array([output])\n",
        "  else:\n",
        "    output = data_cat[cat_feature_name].apply(findCatIndex) \n",
        "    return output.values\n",
        "\n",
        "# function to reformat the input data into the necessary input data for our network to be trained on.\n",
        "def network_input_process(data_num=None, data_cols=None, y_output=None, input_type=1):\n",
        "  # create a list of inputs. \n",
        "  try:\n",
        "    cat_size = data_cols.shape[1]\n",
        "    columns_name = data_cols.columns\n",
        "    col_type = None\n",
        "  except:\n",
        "    cat_size = data_cols.shape[0]\n",
        "    columns_name = data_cols.index\n",
        "    col_type = 1\n",
        "\n",
        "  # define the size of the network list inputs\n",
        "  network_inputs = [None] * (cat_size)\n",
        "\n",
        "  # add inputs to the list\n",
        "  # for the categorical inputs\n",
        "\n",
        "  for i in range(cat_size):\n",
        "    #network_inputs[i] = pd.factorize(data_cols.iloc[:, i].values)[0]\n",
        "    # map each categorical variable to their respective index values\n",
        "    #print (columns_name[i])\n",
        "    #network_inputs[i] = transfromForEmbed(data_cols, cat_feature_name=columns_name[i], cat_mapping=cat_mappings, type=col_type)\n",
        "    network_inputs[i] = data_cols.loc[:, columns_name[i]].values\n",
        "\n",
        "  return network_inputs, y_output.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_hvo4n8st89",
        "colab_type": "code",
        "outputId": "749673c6-e752-4de4-c5bd-bb2f3dfb7f68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "# creating the training inputs\n",
        "scaler = RobustScaler()\n",
        "# scale and fit\n",
        "#scaler.fit(data_t.loc[:, num_features].values)\n",
        "#data_t.loc[:, num_features] = scaler.transform(data_t.loc[:, num_features].values)\n",
        "#data_v.loc[:, num_features] = scaler.transform(data_v.loc[:, num_features].values)\n",
        "\n",
        "# create the requirment for the embedding model\n",
        "XTrain, yTrain = network_input_process(data_num=None, data_cols=data[cat_features], y_output=data.iloc[:,3])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# creating the training inputs\\nscaler = RobustScaler()\\n# scale and fit\\n#scaler.fit(data_t.loc[:, num_features].values)\\n#data_t.loc[:, num_features] = scaler.transform(data_t.loc[:, num_features].values)\\n#data_v.loc[:, num_features] = scaler.transform(data_v.loc[:, num_features].values)\\n\\n# create the requirment for the embedding model\\nXTrain, yTrain = network_input_process(data_num=None, data_cols=data[cat_features], y_output=data.iloc[:,3])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jmf037rcf2H",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXfQPjtd2Heq",
        "colab_type": "text"
      },
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkykfeJQUuB",
        "colab_type": "text"
      },
      "source": [
        "In order to apply embeddings for out categorical variables. We need to first determine which features are categorical variable and which are not. We need to ensure we caputure all possible cases of the categorical features. \n",
        "\n",
        "Okay, then for each categorical variable we need to capture the cardinalty of the feature iteself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCbbzW_yCym1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the embedding category mappings \n",
        "cat_mappings = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', low_memory=False)\n",
        "\n",
        "# Load the embedding model\n",
        "embed_model = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model.h5\")\n",
        "embed_model2 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model2.h5\")\n",
        "embed_model_final = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model_low_mae.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeVmG5ufCkpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fetch out the trained embedding model the model\n",
        "model_embeddings = None\n",
        "model_embeddings = create_model(saved_model=embed_model_final)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BAmSdlgEOSk",
        "colab_type": "text"
      },
      "source": [
        "#### Category Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_XVdZmZsogj",
        "colab_type": "code",
        "outputId": "e4e3060a-ca9f-49fa-ad15-2fa70c4b81fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# load the encoder model\n",
        "encoder_model_embed_85 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_encoder_85.h5\")\n",
        "encoder_model_embed_43 = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_encoder_43.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUIGFrcegUrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function manages the numerical stuffs\n",
        "# given a data (scaled), it will generate a new representation of that data\n",
        "def categorical_pipeline(data_in=None, data_embed_in=None, model_embed=None, model_encoder=None, cat_type=0):\n",
        "\n",
        "  if cat_type == 1:\n",
        "    # means one hotencoding version:\n",
        "    data_out = pd.get_dummies(data_embed_in, drop_first=True)\n",
        "    return data_out.values\n",
        "    \n",
        "  # create the requirment for the embedding model\n",
        "  XTrain, yTrain = network_input_process(data_num=None, data_cols=data_embed_in[cat_features], y_output=data_t_embed.iloc[:,3])\n",
        "\n",
        "  # Embedding Prediction\n",
        "  embed_data = model_embed.predict(XTrain)\n",
        "\n",
        "  # the encoder model as well\n",
        "  encoder_data = model_encoder.predict(embed_data)\n",
        "\n",
        "  # that's all here\n",
        "  return embed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEG2UNcTjX_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Rtz6b6UCTmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dnn_model(input_size):\n",
        "  ### Create the deep leaning model\n",
        "  input_layer1 = Input(shape=(input_size,), name=\"feature_input_layer\")\n",
        "  #encoder_layer1 = Dense(units=85, name='encoder_layer1', activity_regularizer=regularizers.l1(10e-5))(input_encoder)\n",
        "  dense_layer1 = Dense(units=300, name='dense_layer1' )(input_layer1)\n",
        "  activation_1 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer1)\n",
        "  #activation_1 = ReLU()(dense_layer1)\n",
        "  droput_layer1 = Dropout(0.2)(activation_1)\n",
        "\n",
        "  dense_layer2 = Dense(units=300, name='dense_layer2' )(droput_layer1)\n",
        "  #activation_2 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer2_Activation')(dense_layer2)\n",
        "  activation_2 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer2)\n",
        "  #activation_2 = ReLU()(dense_layer2)\n",
        "  droput_layer2 = Dropout(0.2)(activation_2)\n",
        "\n",
        "  dense_layer3 = Dense(units=300, name='dense_layer3' )(droput_layer2)\n",
        "  #activation_3 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer3_Activation')(dense_layer3)\n",
        "  activation_3 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer3)  \n",
        "  #activation_3 = ReLU()(dense_layer3)\n",
        "  droput_layer3 = Dropout(0.2)(activation_3)\n",
        "\n",
        "  dense_layer4 = Dense(units=300, name='dense_layer4')(droput_layer3)\n",
        "  activation_4 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer4)  \n",
        "  #activation_4 = ReLU()(dense_layer4)\n",
        "  droput_layer4 = Dropout(0.2)(activation_4)\n",
        "\n",
        "  dense_layer5 = Dense(units=300, name='dense_layer5' )(droput_layer4)\n",
        "  #activation_3 = PReLU(alpha_initializer=Constant(value=0.2), name='dense_layer3_Activation')(dense_layer3)\n",
        "  activation_5 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer5)  \n",
        "  #activation_5 = ReLU()(dense_layer5)\n",
        "  droput_layer5 = Dropout(0.2)(activation_5)\n",
        "\n",
        "  dense_layer6 = Dense(units=300, name='dense_layer6')(droput_layer5)\n",
        "  activation_6 = PReLU(alpha_initializer=Constant(value=0.2))(dense_layer6)  \n",
        "  #activation_6 = ReLU()(dense_layer6)\n",
        "  droput_layer6 = Dropout(0.2)(activation_6)\n",
        "\n",
        "  # output layer\n",
        "  dense_layer7 = Dense(units=1, name='output_layer')(droput_layer6)\n",
        "\n",
        "  # create the AE model\n",
        "  rul_model = Model(inputs=input_layer1, outputs=dense_layer7, name=\"RUL_Prediction_Model\")\n",
        "\n",
        "  # compile the AE model\n",
        "  rul_model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "  return rul_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gP9ec3KJCF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_results(model_dev=None):\n",
        "  predictions = model_dev.predict(data_test)\n",
        "\n",
        "  # plotting our prediction result\n",
        "  test_predictions = pd.Series(predictions.reshape(data_test_output.shape[0],))\n",
        "  pred_df = pd.DataFrame(data_test_output, columns=['True RUL'])\n",
        "  pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
        "  pred_df.columns = ['True RUL', 'Predicted RUL']\n",
        "\n",
        "  '''\n",
        "  plt.figure()\n",
        "  pred_df[:1000].plot(figsize=(20,10))\n",
        "  plt.xlabel('Data Samples')\n",
        "  plt.ylabel('RUL')\n",
        "  plt.title('Random Forest Prediction on AE Embeddings')\n",
        "  '''\n",
        "\n",
        "  # mean absolute error for our predictions\n",
        "  mae_score = mean_absolute_error(pred_df['True RUL'], pred_df['Predicted RUL'])\n",
        "\n",
        "  # explained variance score\n",
        "  ep_score = explained_variance_score(pred_df['True RUL'].values, pred_df['Predicted RUL'].values)\n",
        "\n",
        "  return mae_score, ep_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMFW3PWrTMB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiyToTSLkgr",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing\n",
        "Here we perform training and testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtksyqRdkEXP",
        "colab_type": "text"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAsTf9DPEY7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chassis_group = data['T_CHASSIS'].values\n",
        "\n",
        "cv_splits = crossValidationSplit(kfolds=5, split=0.2)\n",
        "\n",
        "# filepath for saving best models\n",
        "filepath=\"rul_weights_best.hdf5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPBL2lA4Elpq",
        "colab_type": "code",
        "outputId": "39c00417-793e-442f-879e-576fefb6f7bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "# Performing 5 Fold cross validation\n",
        "cv_splits = crossValidationSplit(kfolds=5, split=0.2)\n",
        "\n",
        "cv_count = 0\n",
        "mae_score_lists = []\n",
        "ep_score_lists = []\n",
        "for train_idx, test_idx in cv_splits.split(data, groups=chassis_group):\n",
        "  cv_count = cv_count + 1\n",
        "  print (\"Kfold Cross Validation Count - \", cv_count)\n",
        "  data_t = data.iloc[train_idx, :].reset_index()\n",
        "  data_v = data.iloc[test_idx, :].reset_index()\n",
        "\n",
        "  ####\n",
        "  data_t_embed = data2.iloc[train_idx, :].reset_index()\n",
        "  data_v_embed = data2.iloc[test_idx, :].reset_index()  \n",
        "  \n",
        "  # define the scaler\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  #### Training Data\n",
        "  # perform scaling on the nummerical features\n",
        "  data_scaled = copy.copy(data_t)\n",
        "  data_scaled[num_features] = scaler.fit_transform(data_t[num_features])\n",
        "\n",
        "  # fetch out the numerical representation\n",
        "  data_num_train = numerical_pipeline(data_scaled, model1=encoder_vanila_50)\n",
        "  #data_num_train = data_scaled[num_features].values\n",
        "  # fetch out the categorical representation\n",
        "  #data_cat_train = categorical_pipeline(data_embed_in=data_t_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n",
        "  # one hot encoding version of the categorical features\n",
        "  \n",
        "  # one hot encoding version of the categorical features\n",
        "  data_one_hot = pd.get_dummies(data[cat_features], drop_first=True)\n",
        "\n",
        "  # split data to train and test - For the embedding informtion\n",
        "  data_t_hot = data_one_hot.iloc[train_idx, :].reset_index()\n",
        "  data_v_hot = data_one_hot.iloc[test_idx, :].reset_index()\n",
        "  data_cat_train = data_t_hot.values\n",
        "  \n",
        "   # merge together\n",
        "  data_train = np.concatenate((data_num_train, data_cat_train), axis=1)\n",
        "  #data_train = data_num_train\n",
        "  data_train_output = data_t['RUL'].values\n",
        "\n",
        "  print ('Training data - ', data_num_train.shape, data_cat_train.shape, data_train.shape)\n",
        "\n",
        "  #### Testing stage\n",
        "  data_scaled = copy.copy(data_v)\n",
        "  data_scaled[num_features] = scaler.transform(data_v[num_features])\n",
        "\n",
        "  # fetch out the numerical representation\n",
        "  data_num_test = numerical_pipeline(data_scaled, model1=encoder_vanila_50)\n",
        "  #data_num_test = data_scaled[num_features].values\n",
        "\n",
        "  # fetch out the categorical representation\n",
        "  #data_cat_test = categorical_pipeline(data_embed_in=data_v_embed, model_embed=model_embeddings, model_encoder=encoder_model_embed_43)\n",
        "  # one hot encoding version of the categorical features\n",
        "  data_cat_test = data_v_hot.values\n",
        "\n",
        "  # merge together\n",
        "  data_test = np.concatenate((data_num_test, data_cat_test), axis=1)\n",
        "  #data_test = data_num_test\n",
        "  data_test_output = data_v['RUL'].values\n",
        "  print ('Testing data - ', data_num_test.shape, data_cat_test.shape, data_test.shape)\n",
        "\n",
        "  ### Evaluation\n",
        "  ''' Random Forest model\n",
        "  regr = RandomForestRegressor(random_state=42, n_jobs=-1, n_estimators=100, criterion='mae', max_depth=100, verbose=1)\n",
        "  regr.fit(data_train, data_train_output)\n",
        "  '''\n",
        "  # DNN model\n",
        "  # create a checkpoint to save only the best model\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_mae', verbose=0, save_best_only=True, mode='min')\n",
        "  rul_model = dnn_model(data_train.shape[1])\n",
        "  early_stop = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=5)\n",
        "  rul_model.fit(x=data_train, y=data_train_output, epochs=100, batch_size=128, verbose=0, validation_data=(data_test, data_test_output), workers=-1, callbacks=[early_stop, checkpoint])\n",
        "\n",
        "  # results\n",
        "  # evaluate using the best model\n",
        "  # load the best weights\n",
        "  rul_model.load_weights(filepath)\n",
        "  rul_model.compile(optimizer='adam', loss='mae', metrics=['mae'])\n",
        "  mae_score, ep_score = model_results(model_dev=rul_model)\n",
        "\n",
        "  print (\"Model Score for Fold - \", cv_count)\n",
        "  print (\"MAE: \", mae_score)\n",
        "  print (\"Explained_Variance: \", ep_score)\n",
        "  # add score to list\n",
        "  mae_score_lists.append(mae_score)\n",
        "  ep_score_lists.append(ep_score)\n",
        "\n",
        "  print (\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Kfold Cross Validation Count -  1\n",
            "Training data -  (16194, 50) (16194, 201) (16194, 251)\n",
            "Testing data -  (3760, 50) (3760, 201) (3760, 251)\n",
            "Epoch 00030: early stopping\n",
            "Model Score for Fold -  1\n",
            "MAE:  157.53100653201975\n",
            "Explained_Variance:  0.47341288681548976\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "Kfold Cross Validation Count -  2\n",
            "Training data -  (15972, 50) (15972, 201) (15972, 251)\n",
            "Testing data -  (3982, 50) (3982, 201) (3982, 251)\n",
            "Epoch 00025: early stopping\n",
            "Model Score for Fold -  2\n",
            "MAE:  179.0046278022151\n",
            "Explained_Variance:  0.3492024088071254\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "Kfold Cross Validation Count -  3\n",
            "Training data -  (16063, 50) (16063, 201) (16063, 251)\n",
            "Testing data -  (3891, 50) (3891, 201) (3891, 251)\n",
            "Epoch 00028: early stopping\n",
            "Model Score for Fold -  3\n",
            "MAE:  176.15314909131814\n",
            "Explained_Variance:  0.31487993406557935\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "Kfold Cross Validation Count -  4\n",
            "Training data -  (16005, 50) (16005, 201) (16005, 251)\n",
            "Testing data -  (3949, 50) (3949, 201) (3949, 251)\n",
            "Epoch 00031: early stopping\n",
            "Model Score for Fold -  4\n",
            "MAE:  182.13628735401022\n",
            "Explained_Variance:  0.29339974415699055\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
            "Kfold Cross Validation Count -  5\n",
            "Training data -  (15969, 50) (15969, 201) (15969, 251)\n",
            "Testing data -  (3985, 50) (3985, 201) (3985, 251)\n",
            "Epoch 00034: early stopping\n",
            "Model Score for Fold -  5\n",
            "MAE:  156.53656177831863\n",
            "Explained_Variance:  0.440543494350764\n",
            "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFkIs3hPJzsJ",
        "colab_type": "code",
        "outputId": "b46dac9c-d644-4e6f-b3ff-6b6f3ac9f7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "########\n",
        "print (\"#######################################\")\n",
        "print (\"Kfold Cross Validation is Complete. Total Folds - \", cv_count)\n",
        "print (\"MAE Average Score is : \", np.mean(mae_score_lists))\n",
        "print (\"MAE List - \", mae_score_lists)\n",
        "print (\"Explained Variance Average Score is : \", np.mean(ep_score_lists))\n",
        "print (\"#######################################\")"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#######################################\n",
            "Kfold Cross Validation is Complete. Total Folds -  5\n",
            "MAE Average Score is :  169.2991469662323\n",
            "MAE List -  [156.37725389352505, 180.08169608880144, 173.96029439360953, 181.9225641483897, 154.15392630683584]\n",
            "Explained Variance Average Score is :  0.37810625830929245\n",
            "#######################################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2QooK-ldQcI",
        "colab_type": "text"
      },
      "source": [
        "[152.79084035589736, 164.67501267699487, 145.9169874509911, 162.08565716949948, 143.87037558226544]\n",
        "153.867774647129\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLxCq8euZDd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYNa0V15ZD9u",
        "colab_type": "text"
      },
      "source": [
        "MAE List -  [143.6790844776529, 152.57317720448654, 150.20180196303752, 148.66132092107608, 141.44549391159]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vkxNOP8Q8J4",
        "colab_type": "text"
      },
      "source": [
        "Kfold Cross Validation is Complete. Total Folds -  5\n",
        "MAE Average Score is :  166.67422428659142\n",
        "MAE List -  [162.3936416305126, 175.5590205381768, 153.3340330050372, 177.5236260111527, 164.56080024807784]\n",
        "Explained Variance Average Score is :  0.384136792505209\n",
        "With early stopping of 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bn43f_nneL",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq-J_V4M0CpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPxJnxD44vHk",
        "colab_type": "text"
      },
      "source": [
        "165.8035277806431 - 6 layers using 300 units"
      ]
    }
  ]
}