{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity Embeddings_v6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1dNbjhBsFetQAx_cu72cb-256k1TR8XEl",
      "authorship_tag": "ABX9TyNm27LDpCvPWuvR5Abc9u0N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thehapyone/Thesis_Project/blob/master/Entity_Embeddings_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBH4RjYX1OGw",
        "colab_type": "text"
      },
      "source": [
        "# Learned Embeddings\n",
        "This program will attempt to create embeddings for the categorical features using the pipelines code developed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7mCtb7J1TX8",
        "colab_type": "code",
        "outputId": "d6165f9f-81cd-4512-952c-40226582d2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc1JPb4v074V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import copy as copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRGnXWAW-rXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('drive/My Drive/Thesis/Collab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVU9EuJE-O1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%run drive/\"My Drive\"/Thesis/\"Collab Notebooks\"/category_pipeline.ipynb\n",
        "#%load"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjSOi7PZ1VFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using the latest updated file\n",
        "#data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v3_Sorted_Raw_Db_with_RUL.csv\", sep=',', low_memory=False)\n",
        "#data = pd.read_csv(\"drive/My Drive/Thesis/data/Feature_Engineering/KNN_Imputed_Datasets/KNN_Imputed_k_40.csv\", sep=',', low_memory=False)\n",
        "data = pd.read_csv(\"drive/My Drive/Thesis/data/Data_Engineering/Edited_Raw_Data/v2_Sorted_Database.csv\", sep=',', low_memory=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu3oJqxA1520",
        "colab_type": "code",
        "outputId": "d2e87ab6-8dff-4dd1-c377-d0fc158dfb90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(336677, 447)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP0LunVOA0up",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9yR1sOn3G2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the Extraction pipeline\n",
        "# first stage of the pipeline\n",
        "from extraction_pipeline import *\n",
        "from category_pipeline import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQdzgdMKByPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a backup\n",
        "original_data = copy.copy(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OslT0fhAvY7",
        "colab_type": "code",
        "outputId": "52e166d3-01b6-4634-9936-a5338580dee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "# remove duplicates\n",
        "#data = removeDuplicates(data, debug=True)\n",
        "data = data.drop(labels=['DELIVERY_DATE.1'], axis=1)\n",
        "print ('After duplicates removal - ',data.shape)\n",
        "\n",
        "# the only duplicate column discovered was - ['DELIVERY_DATE.1']\n",
        "print ('After duplicates removal - ',data.shape)\n",
        "# Remove zero variance\n",
        "data = removeZeroVariance(data, debug=True)\n",
        "print ('After Zero Variance removal - ',data.shape)\n",
        "\n",
        "# drop redundant features\n",
        "data = dropRedundantFeatures(data)\n",
        "print ('After Redundant Drop removal - ',data.shape)\n",
        "\n",
        "cols_left = data.columns\n",
        "'''"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# remove duplicates\\n#data = removeDuplicates(data, debug=True)\\ndata = data.drop(labels=['DELIVERY_DATE.1'], axis=1)\\nprint ('After duplicates removal - ',data.shape)\\n\\n# the only duplicate column discovered was - ['DELIVERY_DATE.1']\\nprint ('After duplicates removal - ',data.shape)\\n# Remove zero variance\\ndata = removeZeroVariance(data, debug=True)\\nprint ('After Zero Variance removal - ',data.shape)\\n\\n# drop redundant features\\ndata = dropRedundantFeatures(data)\\nprint ('After Redundant Drop removal - ',data.shape)\\n\\ncols_left = data.columns\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEc9TSQTatHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VO1pnQzS3Y6",
        "colab_type": "code",
        "outputId": "68e68924-372e-4cca-a1c5-41460258594b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# date extractor\n",
        "data = dateInfoExtractor(raw_data=data, debug=False)\n",
        "print ('After date extractor - ',data.shape)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After date extractor -  (336677, 449)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omgICBK5CRHm",
        "colab_type": "code",
        "outputId": "b42648cf-b396-4117-821d-ab71706d8b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# missing values analysis\n",
        "#\n",
        "print (\"missing values rate before: \", 100 * (data.isnull().sum().sort_values(ascending=False) / data.shape[0]) )\n",
        "data = fillMissingValues(data)\n",
        "print (\"missing values rate after: \", 100 * (data.isnull().sum().sort_values(ascending=False) / data.shape[0]) )\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "missing values rate before:  LX_PSC_P1FUO_NUMBER_OF_STARTER_ACTIVATION_D    43.645096\n",
            "LX_PSC_P1FUN_NUMBER_OF_ENGINE_STARTS_DURING    43.194516\n",
            "LX_PSC_P1I80_TOP_VALUE_OF_ENGINE_SPEED_IN_O     5.833187\n",
            "SLX_PFE_0005_CLUTCH_OVER_MILEAGE                4.542039\n",
            "SLX_PFE_0006_PARK_OVER_MILAGE                   4.388479\n",
            "                                                 ...    \n",
            "VAR_FFX_PUSHER_AXLE_FEATURES                    0.000000\n",
            "VAR_GSX_CHASSIS_HEIGHT                          0.000000\n",
            "VAR_2DX_REAR_CAB_SUSPENSION                     0.000000\n",
            "VAR_2CX_CAB_VERSION                             0.000000\n",
            "DAY                                             0.000000\n",
            "Length: 449, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/impute/_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  \" reached.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "missing values rate after:  LX_PSC_P1I80_TOP_VALUE_OF_ENGINE_SPEED_IN_O    5.833187\n",
            "SLX_PFE_0005_CLUTCH_OVER_MILEAGE               4.542039\n",
            "SLX_PFE_0006_PARK_OVER_MILAGE                  4.388479\n",
            "LX_PSC_P1MNF_COMPRESSOR_LOAD_PHASE_COUNTER     3.922454\n",
            "X_PSC_P1LJQ_ADBLUE_ADAPTION_FACTOR_INITIAL     3.400886\n",
            "                                                 ...   \n",
            "VAR_2RB_BATTERY_MAINTENANCE                    0.000000\n",
            "VAR_DKX_GROSS_COMBINATION_WEIGHT               0.000000\n",
            "VAR_YLX_REAR_SUSPENSION_SYSTEM                 0.000000\n",
            "VAR_QCX_TOPOGRAPHY                             0.000000\n",
            "DAY                                            0.000000\n",
            "Length: 449, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14TT33eD3Qb",
        "colab_type": "text"
      },
      "source": [
        "## Category Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsHVm38hD5V_",
        "colab_type": "code",
        "outputId": "bd5b619f-dec6-4325-dc66-f98ee19b9c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Date transformation\n",
        "data = extraTrasfromToCategory(data, debug=True)\n",
        "\n",
        "'''\n",
        "# Zero Variance removal\n",
        "data = removeZeroCategory(data)\n",
        "print ('After all removal pipeline - ',data.shape)\n",
        "'''"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dtypes are: \n",
            "DAY                                  object\n",
            "MONTH                                object\n",
            "YEAR                                 object\n",
            "VFE_0005_VEHICLE_OPERATION_DIGIT1    object\n",
            "VFE_0008_HAS_PTO                     object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Zero Variance removal\\ndata = removeZeroCategory(data)\\nprint ('After all removal pipeline - ',data.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF2B3mwOqYOD",
        "colab_type": "code",
        "outputId": "2bc989c7-6e0d-4f07-bd41-9d8703aa4926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "'''# First time \n",
        "# find all categorical features.\n",
        "data_cat = data.select_dtypes(include='object')\n",
        "cat_features_df = embeddingSizeExtract(data_categorical=data_cat)\n",
        "cat_features_df\n",
        "\n",
        "# save this embedding dataframe information to a csv file\n",
        "cat_features_df.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_embedding_details.csv\", sep=',', index=False)\n",
        "'''\n",
        "# load this information from the save csv file\n",
        "cat_features_df = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_embedding_details.csv\", sep=',', low_memory=False)\n",
        "cat_features_df"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Cardinality</th>\n",
              "      <th>Embedding_Size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DAY</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MONTH</td>\n",
              "      <td>12</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>YEAR</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>VAR_7MA_FRONT_AXLE_TYPE</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>VAR_DPX_ENGINE_TYPE</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>VFE_0009_LTVF_TYPE</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>VFE_0005_VEHICLE_OPERATION_DIGIT1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>VFE_0006_VEHICLE_OPERATION_DIGIT2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>VFE_0007_CAB_HEIGHT</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>VFE_0008_HAS_PTO</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>74 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Feature  Cardinality  Embedding_Size\n",
              "0                                 DAY            2               1\n",
              "1                               MONTH           12               6\n",
              "2                                YEAR            4               2\n",
              "3             VAR_7MA_FRONT_AXLE_TYPE            2               1\n",
              "4                 VAR_DPX_ENGINE_TYPE            3               2\n",
              "..                                ...          ...             ...\n",
              "69                 VFE_0009_LTVF_TYPE            6               3\n",
              "70  VFE_0005_VEHICLE_OPERATION_DIGIT1            5               3\n",
              "71  VFE_0006_VEHICLE_OPERATION_DIGIT2            9               5\n",
              "72                VFE_0007_CAB_HEIGHT            7               4\n",
              "73                   VFE_0008_HAS_PTO            2               1\n",
              "\n",
              "[74 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3OwJu5BMlcU",
        "colab_type": "code",
        "outputId": "9d6c65b4-314b-4daa-c319-b71c13e2b76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(336677, 449)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCRLRt0TouM",
        "colab_type": "text"
      },
      "source": [
        "(336677, 449)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiyToTSLkgr",
        "colab_type": "text"
      },
      "source": [
        "# Training and Validation Data Extraction\n",
        "Based on the previous understanding. We are going to have to nature of data\n",
        "- Numerical Features and Catgorical Features.\n",
        "The Numerical features have a total of 365 features while the Categorical feature has 72 features.\n",
        "\n",
        "We will have Training and Validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B7KXV8TMb4P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# extracting the categorical features\n",
        "# extracting the categorical features\n",
        "cat_features = cat_features_df['Feature'].values\n",
        "# numerical features are\n",
        "num_features = data.select_dtypes(exclude='object').columns\n",
        "\n",
        "# create a dataframe of the numerical and categorical features and store the result\n",
        "feature_names_cat = pd.DataFrame(data=None)\n",
        "feature_names_num = pd.DataFrame(data=None)\n",
        "feature_names_num['Numerical'] = num_features\n",
        "feature_names_cat['Categorical'] = cat_features\n",
        "\n",
        "# save this feature names of categorical and numerical features to a csv files\n",
        "feature_names_cat.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_cat.csv\", sep=',', index=False)\n",
        "# save this feature names of categorical and numerical features to a csv files\n",
        "feature_names_num.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_num.csv\", sep=',', index=False)\n",
        "'''\n",
        "# load feature names from db\n",
        "feature_names_cat = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_cat.csv\", sep=',', low_memory=False)\n",
        "feature_names_num = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/feature_names_num.csv\", sep=',', low_memory=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CXhVQs-IA5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fetch the categorical and numerical features\n",
        "cat_features = feature_names_cat['Categorical'].values\n",
        "num_features = feature_names_num['Numerical'].values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlkJq-7lPpRw",
        "colab_type": "code",
        "outputId": "380216c1-3fb3-40c8-e93e-1517bdb5d750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_features.shape"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(362,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESuQ3azizCCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
        "# for cross validation, we can use Group KFold spliting - GroupShuffleSplit\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9Y43CX2QWmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for assisting in spliting the data\n",
        "def validationSplit(data_in, split=0.2, toShuffle=False):\n",
        "  # here we will split the data\n",
        "  # splitting by chassis\n",
        "  chassis_data = data_in['T_CHASSIS'].unique()\n",
        "  # splitting the chassis data\n",
        "  c_train, c_val = train_test_split(chassis_data, test_size=split, random_state=42, shuffle=toShuffle)\n",
        "  # extracting out the training and testing\n",
        "  train_data = data_in[data_in['T_CHASSIS'].isin(c_train)].reset_index(drop=True)\n",
        "  test_data = data_in[data_in['T_CHASSIS'].isin(c_val)].reset_index(drop=True)\n",
        "  return train_data, test_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0F8wX64-yKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop na\n",
        "data = data.dropna().reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvmIGc_l5Itv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for assisting in spliting the data\n",
        "def crossValidationSplit(data_in, split=0.2, toShuffle=False):\n",
        "  # here we will split the data based on their chassis grouping\n",
        "  chassis_group = data['T_CHASSIS'].values\n",
        "  ## generating the Group Parameters\n",
        "  c_split = GroupShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
        "  '''\n",
        "  # splitting the data\n",
        "  for train_idx, test_idx in c_split.split(data.values, groups=chassis_group):\n",
        "    print(\"TRAIN:\", data.values[train_idx].shape, \"TEST:\", data.values[test_idx].shape)\n",
        "  '''\n",
        "  return c_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haaA1p20Q61b",
        "colab_type": "text"
      },
      "source": [
        "### Train Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s41BsPBlQ9pz",
        "colab_type": "code",
        "outputId": "1dcce311-7d3a-4835-d564-d0f89ceec8f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_t, data_v = validationSplit(data, split=0.2)\n",
        "data_t.shape, data_v.shape"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((232099, 449), (59494, 449))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkZbVVhbQCU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_t.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/chassis_testing_data.csv\", sep=',', index=False)\n",
        "#data_v.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/chassis_training_data.csv\", sep=',', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twp6hyyLNLHg",
        "colab_type": "text"
      },
      "source": [
        "## Category Summary\n",
        "In summary, we have a total of 74 categories. They can be used to filter new data sources instead of going through the whole extaction and category pipeline all over again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyyKppBvVfuA",
        "colab_type": "text"
      },
      "source": [
        "## Handling numerical features\n",
        "We have a total of 362 numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXfQPjtd2Heq",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkykfeJQUuB",
        "colab_type": "text"
      },
      "source": [
        "In order to apply embeddings for out categorical variables. We need to first determine which features are categorical variable and which are not. We need to ensure we caputure all possible cases of the categorical features. \n",
        "\n",
        "Okay, then for each categorical variable we need to capture the cardinalty of the feature iteself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAZb5vWk2Je9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use tensorflow version 2\n",
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-PSyCt2P6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Reshape, Concatenate, Dense, Flatten, Dropout\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8_MlT2hIjbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.summary import create_file_writer\n",
        "#  TensorFlow and the TensorBoard HParams plugin:\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dpOOuoP4VqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Write metadata file for embeddings\n",
        "def write_metadata(filename, labels):\n",
        "    \"\"\"\n",
        "            Create a metadata file image consisting of sample indices and labels\n",
        "            :param filename: name of the file to save on disk\n",
        "            :param shape: tensor of labels\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"Index\\tLabel\\n\")\n",
        "        for index, label in enumerate(labels):\n",
        "            f.write(\"{}\\t{}\\n\".format(index, label))\n",
        "\n",
        "    print('Metadata file saved in {}'.format(filename))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnxmI0JS5LB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_t[cat_features].nunique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Lt2Sz76c3e",
        "colab_type": "code",
        "outputId": "cc4ddcfc-aa9b-4cf9-e5d3-b02ad8aac5b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_t['VFE_0007_CAB_HEIGHT'].unique()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CABH215', 'CABH165', 'CABH230', 'CABH175', 'CABH210', 'CABH225',\n",
              "       'Low'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzbfaxgV5CeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_col_labels = pd.factorize(data_t.loc[:, 'VFE_0007_CAB_HEIGHT'].values)[0]\n",
        "metadata_path = \"drive/My Drive/Thesis/data/Embedding_Data/metadata_CAB_HEIGHT.tsv\"\n",
        "#write_metadata(metadata_path, feature_col_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJoBX4JQ9j6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_meta = {'Embedding_layer_VFE_0007_CAB_HEIGHT':metadata_path}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ7QrIuDcXd6",
        "colab_type": "text"
      },
      "source": [
        "### Saving and Loading Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQrVg012oYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Goal\n",
        "'''\n",
        "Write a function, when given a categorical variable, it will generate the vector representation for that variable.\n",
        "In order to achieve that, we first need to map very categorical variable to some numerical index value. This value \n",
        "has to be unique and not random. \n",
        "'''\n",
        "def embedTransform(model, variable, cat_feature_name, cat_mapping):\n",
        "  # Given a categorical variable, produce the embedded vector\n",
        "  # fetch the embeddings weights\n",
        "  pre_embedding = \"Embedding_layer_\"\n",
        "  embed_layer = model.get_layer(name=pre_embedding+cat_feature_name)\n",
        "  # Find the index of the categorical variable in the cat_mapping dataframe\n",
        "  cat_index = int(cat_mapping[cat_mapping[cat_feature_name] == variable][cat_feature_name+'_index'].values[0])\n",
        "  embedd_vector = embed_layer.get_weights()[0][cat_index]\n",
        "  return embedd_vector\n",
        "\n",
        "def createCatMapping(data_cat):\n",
        "  # this function will create a mapping dataframe\n",
        "  cat_size = data_cat.shape[1]\n",
        "  feature_names = data_cat.columns\n",
        "\n",
        "  cat_mapping = pd.DataFrame()\n",
        "\n",
        "  col_mapping_list = list()\n",
        "  col_mapping_cols = list()\n",
        "  # try for all dataframes\n",
        "  for i in range(cat_size):\n",
        "    col_indexs = list(pd.factorize(data_cat.iloc[:, i].unique())[0])\n",
        "    # save to list\n",
        "    col_mapping_list.append(list(data_cat.iloc[:, i].unique()))\n",
        "    col_mapping_list.append(col_indexs)\n",
        "    # save the column name as well\n",
        "    col_mapping_cols.append(feature_names[i])\n",
        "    col_mapping_cols.append(feature_names[i]+'_index')\n",
        "\n",
        "  # now we need to save the list to a dataframe\n",
        "  cat_mapping = pd.DataFrame(col_mapping_list)\n",
        "  # transpose it\n",
        "  cat_mapping = cat_mapping.transpose()\n",
        "  # update the columns names\n",
        "  cat_mapping.columns = col_mapping_cols\n",
        "\n",
        "  # save to the dataframe\n",
        "  cat_mapping.to_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', index=False)\n",
        "\n",
        "#createCatMapping(data[cat_features])\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCbbzW_yCym1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load from file\n",
        "cat_mappings = pd.read_csv(\"drive/My Drive/Thesis/data/Embedding_Data/cat_mappings.csv\", sep=',', low_memory=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYu4l6hzUaRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bae8e58f-da46-43bf-b44f-970ae13b22c6"
      },
      "source": [
        "data_t['VFE_0007_CAB_HEIGHT'].unique()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CABH215', 'CABH165', 'CABH230', 'CABH175', 'CABH210', 'CABH225',\n",
              "       'Low'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s75tA5dU1H3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c283e196-63e4-4967-afa1-e728c4c3c2a3"
      },
      "source": [
        "# load the embedding model\n",
        "from tensorflow.keras.models import load_model\n",
        "# identical to the previous one\n",
        "embed_model = load_model(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model.h5\")\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTvqsge8UG5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the embedding vector for feature CABH210 in VFE_0007_CAB_HEIGHT\n",
        "'''\n",
        "my_vector1 = embedTransform(embed_model, variable='CABH175', cat_feature_name='VFE_0007_CAB_HEIGHT', cat_mapping=cat_mappings)\n",
        "my_vector2 = embedTransform(embed_model, variable='CABH165', cat_feature_name='VFE_0007_CAB_HEIGHT', cat_mapping=cat_mappings)\n",
        "my_vector3 = embedTransform(embed_model, variable='CABH210', cat_feature_name='VFE_0007_CAB_HEIGHT', cat_mapping=cat_mappings)\n",
        "my_vector4 = embedTransform(embed_model, variable='CABH230', cat_feature_name='VFE_0007_CAB_HEIGHT', cat_mapping=cat_mappings)\n",
        "my_vector5 = embedTransform(embed_model, variable='Low', cat_feature_name='VFE_0007_CAB_HEIGHT', cat_mapping=cat_mappings)\n",
        "\n",
        "x1 = my_vector4\n",
        "x2 = my_vector5\n",
        "\n",
        "# calculating the cosing similarity of two vectors in the orginal space\n",
        "a_dot = np.dot(x1, x2)\n",
        "similar = a_dot / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
        "similar\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH2htn2kekMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# betwen CABH175 and CABH165, i have 0.867487\n",
        "# betwen CABH175 and CABH210, i have 0.11280868\n",
        "# betwen CABH175 and CABH230, i have -0.5284594\n",
        "# betwen CABH175 and Low, i have -0.36939222\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmpTjqtodg-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f26c0eb-3c9e-425e-d591-1f49fd73496e"
      },
      "source": [
        ""
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94751537"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptDS4G3lUhJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dbb7f929-9780-4d97-d2f9-340cd043f64d"
      },
      "source": [
        "ddsad = embed_model.get_layer(name='Embedding_layer_VFE_0007_CAB_HEIGHT')\n",
        "ddsad.get_weights()"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.14905094, -0.11075342, -0.05900112, -0.0247627 ],\n",
              "        [ 0.02609651, -0.06145609,  0.13505827, -0.00065673],\n",
              "        [-0.06022814, -0.05502219, -0.04270122, -0.07960243],\n",
              "        [ 0.21226509, -0.04694704,  0.28343254,  0.00852349],\n",
              "        [ 0.04596679,  0.11171044,  0.00124936,  0.05315955],\n",
              "        [ 0.0416726 , -0.0106741 ,  0.12163171, -0.02237656],\n",
              "        [-0.04100224, -0.0463232 , -0.00980473, -0.0373629 ]],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRD6S-h7MMps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "dc24ac13-b530-4a07-b6b8-3a2447724a29"
      },
      "source": [
        "gfggg"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-150-ccc88196cc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgfggg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gfggg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pekm8BKhFocu"
      },
      "source": [
        "### Creating the Keras Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3ZS29ptVFn6s",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF1787pgD22Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKSt_HeoNnGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1, embeddings_metadata=embeddings_meta, write_images=True)\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1, embeddings_freq=1,\n",
        "                                   write_images=False)\n",
        "\n",
        "'''\n",
        "# Tensorboard HP Parameters tunnings\n",
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([1024, 512, 362, 256]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.1, 0.2, 0.5]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'rmsprop', 'sgd']))\n",
        "\n",
        "METRIC_MAE = 'mae'\n",
        "\n",
        "with create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
        "    metrics=[hp.Metric(METRIC_MAE, display_name='MAE')],\n",
        "  )\n",
        "\n",
        "  #hp.KerasCallback(logdir, hparams),  # log hparams\n",
        "  #hp.KerasCallback(writer='logs/hparam_tuning', hparams=myhparams)\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6JUt7abZXbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Creating the Keras Embeddings Mode\n",
        "\n",
        "def create_model():\n",
        "  # IDs representing 1-hot encodings\n",
        "  # Need to create the input for all features\n",
        "  cat_embedding_input_layers = []\n",
        "\n",
        "  cat_reshape_layers = []\n",
        "  # Embeddings for the first cat feature\n",
        "  id_feature = 0\n",
        "  # create an array of embeddings layers\n",
        "  cat_embedding_layers = []\n",
        "\n",
        "  #model.add(Reshape(target_shape=(embedding_size,)))\n",
        "\n",
        "  # interate through categorical varaibles\n",
        "  for id_feature in range(cat_features_df.shape[0]):\n",
        "    # creating the input layer for the embeddings\n",
        "    input_layer = Input(shape=(1,), name=\"Input_layer_\"+cat_features_df.loc[id_feature, 'Feature'])\n",
        "    cat_embedding_input_layers.append(input_layer)\n",
        "    # embedding size \n",
        "    layer_embedding_size = cat_features_df.loc[id_feature, 'Embedding_Size']\n",
        "    # create the embedding layers\n",
        "    embedded_layer = Embedding(input_dim=cat_features_df.loc[id_feature, 'Cardinality'], output_dim=layer_embedding_size, name=\"Embedding_layer_\"+cat_features_df.loc[id_feature, 'Feature'], input_length = 1)(input_layer)\n",
        "    cat_embedding_layers.append(embedded_layer)\n",
        "    # add a reshape of the embedding layers\n",
        "    reshape_layer = Reshape(target_shape=(layer_embedding_size,))(embedded_layer)\n",
        "    # appends the rehshape models together\n",
        "    cat_reshape_layers.append(reshape_layer)\n",
        "\n",
        "  ### Create another model to handle the numerical features\n",
        "  ## using all the numerical features\n",
        "  # I have 362 numerical features\n",
        "  numerical_features = num_features.shape[0]\n",
        "  # the input layer for the numerical eat\n",
        "  input_layer_num = Input(shape=(numerical_features), name=\"Num_Input_layer\")\n",
        "  # create the dense layer\n",
        "  num_dense_1 = Dense(units=362, activation='elu', name='Num_Dense_Layer_1')(input_layer_num)\n",
        "\n",
        "  # Concantente all the input layers together as one\n",
        "  all_input_layers = copy.copy(cat_embedding_input_layers)\n",
        "  all_input_layers.append(input_layer_num)\n",
        "\n",
        "  ### Create A combined embedding layers only\n",
        "  combined_emb = Concatenate(axis=1, name='combined_embeddings')(cat_reshape_layers)\n",
        "\n",
        "  ### Creating a merge layers for categorical and numerical\n",
        "  # now create a combined embedding layers with the numerical dense layer\n",
        "  # Combined all the embedding layer and Numerical dense layer together\n",
        "  combined_layers_all = Concatenate(axis=1, name='combined_layers_all')([combined_emb, num_dense_1])\n",
        "\n",
        "  # create a dense layer for the merge layers\n",
        "  dense_layer_1 = Dense(units=1024, activation='elu', name='1st_Dense_Merge')(combined_layers_all)\n",
        "  dropout_layer1_a = Dropout(rate=0.1, name='Dropout_Layer1_a')(dense_layer_1)\n",
        "  # create a dense layer for the merge layers\n",
        "  dense_layer_1_b = Dense(units=1024, activation='elu', name='1st_Dense_Merge_b')(dropout_layer1_a)\n",
        "\n",
        "  '''\n",
        "  So once we have the individual models merged into a full model, we can add layers on top of it network and train it.\n",
        "  '''\n",
        "\n",
        "  # create a dense layer for the merge layers\n",
        "  # add a dropout \n",
        "  dropout_layer1_b = Dropout(rate=0.1, name='Dropout_Layer1_b')(dense_layer_1_b)\n",
        "  dense_layer_2 = Dense(units=1024, activation='elu', name='2nd_Dense_Merge')(dropout_layer1_b)\n",
        "  dropout_layer2_a = Dropout(rate=0.1, name='Dropout_Layer2_a')(dense_layer_2)\n",
        "  dense_layer_2_b = Dense(units=1024, activation='elu', name='2nd_Dense_Merge_b')(dropout_layer2_a)\n",
        "  dropout_layer2_b = Dropout(rate=0.1, name='Dropout_Layer2_b')(dense_layer_2_b)\n",
        "  dense_layer_3 = Dense(units=1024, activation='elu', name='3rd_Dense_Merge')(dense_layer_2_b)\n",
        "  dropout_layer3 = Dropout(rate=0.1, name='Dropout_Layer3')(dense_layer_3)\n",
        "  dense_layer_3_b = Dense(units=1024, activation='elu', name='3rd_Dense_Merge_b')(dropout_layer3)\n",
        "  #dropout_layer3 = Dropout(rate=0.4, name='Dropout_Layer3')(dense_layer_3_b)\n",
        "\n",
        "  # the output layer\n",
        "  # for a regression problem\n",
        "  dense_layer_output = Dense(units=1, name='Output_layer')(dense_layer_3_b)\n",
        "  # for a classification problem\n",
        "  #dense_layer_output = Dense(units=1, activation='sigmoid', name='Output_layer')(dropout_layer3)\n",
        "\n",
        "  ### creating the Full model (embedding plus numerical)\n",
        "  model_emb = Model(inputs=all_input_layers, outputs=dense_layer_output, name=\"Categorical_Embeddings\")\n",
        "\n",
        "  # compile the model - for a regression model\n",
        "  model_emb.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_error'])\n",
        "  # compile the model - for a classification problem\n",
        "  # model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "  return model_emb\n",
        "\n",
        "def train_model(model, x_train, y_train, x_test, y_test, epochs, batch_size, early_stop_callback=None, tensorboard_callback=None):\n",
        "  # fitting the model\n",
        "  #model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_data=(XTest, yTest), callbacks=[early_stop, tensorboard_callback])\n",
        "  model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=batch_size, verbose=2, validation_data=(x_test, y_test), callbacks=[early_stop_callback, tensorboard_callback])\n",
        "  # evaluate the model\n",
        "  _, metric_mae = model.evaluate(x_test, y_test)\n",
        "  return metric_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WFSz0kSuuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For each run, log an hparams summary with the hyperparameters and final accuracy:\n",
        "def run(x_train, y_train, x_test, y_test, epochs, batch_size, early_stop_callback, tensorboard_log):\n",
        "  model = None\n",
        "  model = create_model()\n",
        "  metric_mae = train_model(model, x_train, y_train, x_test, y_test, epochs, batch_size, early_stop_callback=early_stop_callback, tensorboard_callback=tensorboard_log)\n",
        "  # do something with the metrics\n",
        "  return model, metric_mae"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U5QWCikuL4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the model\n",
        "#plot_model(model=model_emb, to_file='drive/My Drive/Thesis/data/Embedding_Data/model_image.png', show_shapes=True, dpi=300)\n",
        "#plot_model(model=model_emb, to_file='drive/My Drive/Thesis/data/Embedding_Data/model_new.png', show_shapes=True, dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hW4daKSQjJsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model_emb.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeLd1zwzoXat",
        "colab_type": "text"
      },
      "source": [
        "## Inputs to the Network\n",
        "\n",
        "We need to create a list of inputs, such that each categorical feature has it's own list of values and the last list will be a 2D array of the continous values. As shown above.\n",
        "\n",
        "The size of the list of inputs : ip = n_cat + 1\n",
        "\n",
        "Remember for each of the embedding network we had set input-size =1 we are taking 1 value each from all the list (except the last list) and sending it to the combined network for training. For the last list, each value itself is a list having the other columns values, and this is sent to the models_rest network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbuVc8kGWYqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUlRi7_CozT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to reformat the input data into the necessary input data for our network to be trained on.\n",
        "def network_input_process(data_num, data_cols, y_output):\n",
        "  # create a list of inputs. \n",
        "  cat_size = data_cols.shape[1]\n",
        "  # define the size of the network list inputs\n",
        "  network_inputs = [None] * (cat_size + 1)\n",
        "\n",
        "  # add inputs to the list\n",
        "  # for the categorical inputs\n",
        "  for i in range(cat_size):\n",
        "    network_inputs[i] = pd.factorize(data_cols.iloc[:, i].values)[0]\n",
        "\n",
        "  # for the continuous value inputs\n",
        "  network_inputs[-1] = data_num.iloc[:, :].values\n",
        "\n",
        "  # that should be all\n",
        "  return network_inputs, y_output.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_hvo4n8st89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating the training inputs\n",
        "scaler = StandardScaler()\n",
        "# scale and fit\n",
        "scaler.fit(data_t.loc[:, num_features].values)\n",
        "data_t.loc[:, num_features] = scaler.transform(data_t.loc[:, num_features].values)\n",
        "data_v.loc[:, num_features] = scaler.transform(data_v.loc[:, num_features].values)\n",
        "\n",
        "Xtrain, yTrain = network_input_process(data_num=data_t[num_features], data_cols=data_t[cat_features], y_output=data_t['RUL'])\n",
        "XTest, yTest = network_input_process(data_num=data_v[num_features], data_cols=data_v[cat_features], y_output=data_v['RUL'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZCa-El4Fgta",
        "colab_type": "text"
      },
      "source": [
        "## Training the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTOAM7OnIZjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_mean_absolute_error', mode='min', verbose=1, patience=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfraLc7vJsyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fitting the model\n",
        "# model_emb.fit(x=Xtrain, y=yTrain, epochs=600, batch_size=64, verbose=2, validation_data=(XTest, yTest), callbacks=[early_stop, tensorboard_callback])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpcFWB0LWziQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove old logs\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2T9WJ4s3965",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the model\n",
        "my_model, metric_result = run(Xtrain, yTrain, XTest, yTest, 300, 64, early_stop, tensorboard_log=tensorboard_callback)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O4gshp89SO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print ('result - ', metric_result)\n",
        "# see model summary\n",
        "#my_model.summary()\n",
        "\n",
        "# save the model\n",
        "#my_model.save(\"drive/My Drive/Thesis/data/Embedding_Data/embedding_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq6xI1chHiYF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_layerss = my_model.get_layer(index=111)\n",
        "\n",
        "ddsad = my_model.get_layer(name='Embedding_layer_VFE_0007_CAB_HEIGHT')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYQe7-IbM3vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b8fe1444-f697-49aa-aa35-0c4f57bb9ee5"
      },
      "source": [
        "data_t['VFE_0007_CAB_HEIGHT'].unique()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CABH215', 'CABH165', 'CABH230', 'CABH175', 'CABH210', 'CABH225',\n",
              "       'Low'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IY5hZyf68Y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddsad.get_config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnxduKbO6rh8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddsad.embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pWm1eXjcXwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ddsad.get_weights()[0][6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AkE4BlYesX7",
        "colab_type": "text"
      },
      "source": [
        "'''\n",
        "First iteration \n",
        "using 512 - 256 - 128\n",
        "###\n",
        "error 202/202 - 3s - loss: 4807.8838 - val_loss: 263093.3125\n",
        "Epoch 71/600\n",
        "202/202 - 3s - loss: 4637.8784 - val_loss: 352122.9062\n",
        "Epoch 00071: early stopping\n",
        "mean-absolute error - 245.14920879082842\n",
        "e_var = -4.4\n",
        "/---------------------\n",
        "using 1028 - 512 - 128\n",
        "\n",
        "\n",
        "----------------------\n",
        "202/202 - 4s - loss: 828.3008 - val_loss: 443548.0000\n",
        "-----------------------\n",
        "Epoch 57/600\n",
        "202/202 - 5s - loss: 5574.1982 - val_loss: 124869.9609\n",
        "------------------------\n",
        "With ELU : 202/202 - 4s - loss: 2499.8364 - val_loss: 66855.7031\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pphSC8HS5eMs",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3aKNQRrBIom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the loss\n",
        "lossess = pd.DataFrame(my_model.history.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLrsm7FMGaJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QkigqmEBL6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossess.plot(figsize=(12,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHeA8TH40y8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lossess['val_loss'].plot(figsize=(12,6))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOhitKlmazF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluating our model\n",
        "np.sqrt(model_emb.evaluate(x=Xtrain, y=yTrain, verbose=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZSgmwvf3xuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluating our model\n",
        "np.sqrt(model_emb.evaluate(x=XTest, y=yTest, verbose=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BC0uHIZbmw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the model\n",
        "test_predicitons = model_emb.predict(XTest)\n",
        "test_predicitons.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm6BnKDEbv3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plotting our prediction result\n",
        "test_predictions = pd.Series(test_predicitons.reshape(2978,))\n",
        "pred_df = pd.DataFrame(yTest, columns=['True RUL'])\n",
        "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
        "pred_df.columns = ['True RUL', 'Predicted RUL']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFNSD5iFcAo2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa6F9FfYcMAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, explained_variance_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS5sojBscaUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mean absolute error for our predictions\n",
        "mean_absolute_error(pred_df['True RUL'], pred_df['Predicted RUL'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIPpLSfwcjUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# so is our error good or bad?\n",
        "data['RUL'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gdYR0KccqjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# explained variance score\n",
        "explained_variance_score(pred_df['True RUL'].values, pred_df['Predicted RUL'].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EexWB-UVeXJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill 1142"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhPzy8uitSoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###\n",
        "# running tensorboard\n",
        "tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}